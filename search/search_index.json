{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLSysOps Framework","text":"<p>The MLSysOps framework operates in the context of a heterogeneous, multi-layered computing continuum, ranging from  centralized cloud infrastructures to resource-constrained far-edge devices. The objective of the framework is to enable autonomic, explainable, and adaptive system management by leveraging artificial intelligence, with minimal human intervention.</p> <p></p> <p>The design of MLSysOps is guided by a system model that introduces the concept of a slice \u2014 a logical grouping of computing, networking, and storage resources across the continuum that is managed as a unit. Each slice is governed by its own  deployment of the MLSysOps control plane and encompasses physical or virtualized resources at different layers.</p> <p>In essence, the framework operates as an abstraction middleware between the participating entities.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Kubernetes deployment management.</li> <li>Multi-cluster management using Karmada.</li> <li>Dynamically configured telemetry system.</li> <li>Plugin systems for configuration policies and mechanisms.</li> <li>Application deployment model, using Kubernetes Custom Resource Definitions (CRDs).</li> <li>System infrastructure inventory, using Kubernetes Custom Resource Definition (CRDs).</li> <li>REST API endpoints (Northbound API service). It can be used by a proprietary CLI.</li> <li>ML Connector service, for easy ML Model management, deployment, retraining, and explainability.</li> <li>Node level management.</li> <li>Deploy using different container runtimes. </li> <li>Resource contrainted devices management (Far-Edge devices).</li> <li>Storage service managed by the framework.</li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<ul> <li>Optimize the deployment of an application in a continuum system slice, using smart policies, that can make use of ML models.</li> <li>Achieve application targets, specified in the application deployment description.</li> <li>Optimize system targets, based on system descriptions.</li> <li>Implement arbitrary configuration mechanisms, and expose them to configuration policies.</li> </ul>"},{"location":"#current-support","title":"Current support","text":"Feature Status Stability Kubernetes Management Alpha Multi-cluster deployment Alpha Multi-cluster networking - Dynamic telemetry system Alpha Plugin system Alpha Application &amp; System descriptions Alpha Basic ML management Alpha Node level management - Far-edge devices - Managed Storage service - <p>Mechanism Plugins</p> Plugin Name Description Configuration options Fluidity Provides the capability to manage Kubernetes pods and services. * Deploy/remove/relocate components <p>Policy Plugins</p> Plugin Name Description staticPlacedComponents It provides the logic to place components."},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Contributing</li> <li>Website</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":""},{"location":"installation/","title":"Installation","text":"<p>This document guides you through the installation of the <code>MLSysOps framework</code> and all required components for executing all supported scenarios.</p> <p>We assume a vanilla ubuntu 22.04 environment, although the <code>MLSysOps framework</code> is able to run on a number of distros.</p> <p>We will be installing and setting up each component individually:</p>"},{"location":"installation/#core-framework","title":"Core Framework","text":""},{"location":"installation/#mlsysops-framework-installation","title":"MLSysOps Framework Installation","text":"<p>The main prerequisite is that there is Karmada instance installed, with at least one Kubernetes cluster registered. We assume that the Karmada is installed in a standalone cluster. Karmada instance should include the <code>karmada-search</code> plugin.</p> <p>MLSysOps Framework consists of three main components called MLSysOps Agents. These components require the following services to operate before starting:</p> <ul> <li>Ejabberd XMPP Server</li> <li>Redis</li> <li>Docker installed in Karmada-Management VM</li> </ul> <p>There are two services that provide additional functionalities to the user:</p> <ul> <li>Northbound API: This service is part of the MLSysOps agents. It provides endpoints for controlling the components and behaviors of the agents.</li> <li>ML Connector: This service is responsible for managing and deploying Machine Learning models. It exposes its functionality through a separate API.</li> </ul> <p>To ensure the correct bootstrap, the agents should start in the following order: 1. Continuum agent 2. Cluster agent  3. Node agents</p> <p>All the deployments take place in a Kubernetes cluster, in separate namespace 'mlsysops-framework'. All the third-party services, as well as the Continuum agent are deployed in the managament cluster, the same that is installed in karmada host.</p>"},{"location":"installation/#system-descriptions-preparation","title":"System descriptions preparation","text":"<p>Before the installation process takes place, system descriptions for every layer must be prepared. A system description is a YAML file, implemented as Kubernetes CRDs. Examples can be found in <code>descriptions/</code> directory. The descriptions for each layer reside in the respectively named directory: continuum, clusters, nodes. Each file MUST have the name of the corresponding hostname, followed by the .yaml or .yml suffix. For example, a machine at the node level, with hostname <code>node-1</code>, should have a description file named <code>node-1.yaml</code> under the directory <code>nodes/</code>.</p> <ul> <li>Continuum level descriptions, require one single file, that declare the continuumID and the clusters that we allow MLSysOps to manage.</li> <li>Cluster level descritptions, require a file for each cluster registered in Karmada. It contains the clusterID and a list of node hostnames, that MLSysOps is allowed to manage.</li> <li>Node level descriptions, contain the detailed information about the node resources. Example here.</li> </ul>"},{"location":"installation/#option-1-automated-deployment","title":"Option 1: Automated Deployment","text":"<p>MLSysOps CLI tool can be used to automatically deploy all the necessary components. It needs the kubeconfigs of Karmada host cluster and Karmada API.</p> <ul> <li><code>export KARMADA_HOST_KUBECONFIG=&lt;path to karmada host kubeconfig&gt;</code></li> <li><code>export KARMADA_API_KUBECONFIG=&lt;path to karmada api kubeconfig&gt;</code></li> <li><code>export KARMADA_HOST_IP=&lt;karmada host ip&gt;</code></li> </ul> <p>And then execute the CLI command inside <code>deployments</code> directory, with <code>descriptions</code> directory files prepared: - <code>python3 deploy.py</code></p>"},{"location":"installation/#option-2-manual-deployment","title":"Option 2: Manual Deployment","text":""},{"location":"installation/#continuum-management-cluster","title":"Continuum - Management Cluster","text":"<p>In Karmada host cluster: <code>export KUBECONFIG=&lt;karmada host kubeconfig&gt;</code></p> <ul> <li>Create namespace</li> <li> <p><code>kubectl apply -f namespace.yml</code></p> </li> <li> <p>Install Required services</p> </li> <li>Change <code>POD_IP</code> to Karmada host IP in <code>xmpp/deployment.yaml</code>.</li> <li><code>kubectl apply -f xmpp/deployment.yaml</code></li> <li>Change <code>{{ KARMADA_HOST_IP }}</code> to Karmada host IP in <code>api-service-deployment.yaml</code>.</li> <li><code>kubectl apply -f api-service-deployment.yaml</code></li> <li>Change <code>{{ KARMADA_HOST_IP }}</code> to Karmada host IP in <code>redis-stack-deployment.yaml</code>.</li> <li><code>kubectpl apply -f redis-stack-deployment.yaml</code></li> <li> <p><code>docker compose up -d -f &lt;mlconnector.docker-composer.yaml&gt;</code></p> </li> <li> <p>Apply RBAC</p> </li> <li> <p><code>kubectl apply -f mlsysops-rbac.yaml</code></p> </li> <li> <p>Attach Karmada API kubeconfig as ConfigMap</p> </li> <li><code>kubectl create configmap continuum-karmadapi-config --from-file=&lt;path/to/local/karmada-api.kubeconfig&gt; --namespace=mlsysops-framework</code></li> <li></li> <li>Attach Continuum system description as ConfigMap</li> <li> <p><code>kubectl create configmap continuum-system-description --from-file=&lt;descriptions/continuum/hostname.yaml&gt; --namespace=mlsysops-framework</code></p> </li> <li> <p>Start continuum agent</p> </li> <li><code>kubectl apply -f continuum-agent-daemonset.yaml</code></li> </ul> <p>In Karmada API Server: <code>export KUBECONFIG=&lt;karmada api server kubeconfig&gt;</code></p>"},{"location":"installation/#cluster-deployments","title":"Cluster deployments","text":"<ul> <li>Setup Karmada propagation policies</li> <li><code>kubectl apply -f cluster-propagation-policy.yaml</code></li> <li> <p><code>kubectl apply -f propagation-policy.yaml</code></p> </li> <li> <p>Create the namespace</p> </li> <li> <p><code>kubectl apply -f namespace.yaml</code></p> </li> <li> <p>Apply RBAC</p> </li> <li> <p><code>kubectl apply -f mlsysops-rbac.yaml</code></p> </li> <li> <p>Create a configmap based on the system description, using as the namefile, the hostname of the cluster manage node hostname</p> </li> <li> <p><code>kubectl create configmap cluster-system-description --from-file=descriptions/clusters --namespace=mlsysops-framework</code></p> </li> <li> <p>Apply the daemonset YAML file</p> </li> <li>Change <code>{{ KARMADA_HOST_IP }}</code> to Karmada host IP in <code>cluster-agents-daemonset.yaml</code>. </li> <li><code>kubectl apply -f cluster-agents-daemonset.yaml</code></li> </ul>"},{"location":"installation/#nodes-deployment","title":"Nodes deployment","text":"<ul> <li>Namespaces and RBAC were created with the cluster setup.</li> <li>Prepare the system description, for each node, and name each file with the host name. </li> <li>Create a configmap based on the system description, for each node.</li> <li><code>kubectl create configmap node-system-descriptions --from-file=descriptions/nodes --namespace=mlsysops-framework</code></li> <li>env variable.</li> <li>Apply the daemonset YAML file</li> <li><code>kubectl apply -f node-agents-daemonset.yaml</code></li> </ul> <p>Note: Be aware that some instructions might override existing tools and services.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This document acts as a quickstart guide to showcase indicative features of the <code>MLSysOps Framework</code>. Please refer to the installation guide for more detailed installation instructions, or the design document for more details regarding <code>MLSysOps</code>'s architecture.</p>"},{"location":"quickstart/#mlsysops-framework-installation","title":"MLSysOps Framework Installation","text":"<p>The main prerequisite is that there is Karmada instance installed, with at least one Kubernetes cluster registered. We assume that the Karmada is installed in a standalone cluster. Karmada instance should include the <code>karmada-search</code> plugin. You can follow the instructions in Testbed installation to create the appropriate environment.</p> <p>MLSysOps Framework consists of three main components called MLSysOps Agents. These components require the following services to operate before starting:</p> <ul> <li>Ejabberd XMPP Server</li> <li>Redis</li> <li>Docker installed in Karmada-Management VM</li> </ul> <p>There are two services that provide additional functionalities to the user:</p> <ul> <li>Northbound API: This service is part of the MLSysOps agents. It provides endpoints for controlling the components and behaviors of the agents.</li> <li>ML Connector: This service is responsible for managing and deploying Machine Learning models. It exposes its functionality through a separate API.</li> </ul> <p>To ensure the correct bootstrap, the agents should start in the following order:</p> <ol> <li>Continuum agent</li> <li>Cluster agent </li> <li>Node agents</li> </ol> <p>All the deployments take place in a Kubernetes cluster, in separate namespace 'mlsysops-framework'. All the third-party services, as well as the Continuum agent are deployed in the managament cluster, the same that is installed in karmada host.</p>"},{"location":"quickstart/#step-1-clone-the-repo","title":"Step 1: Clone the repo","text":"<p><code>git clone https://github.com/mlsysops-eu/mlsysops-framework</code></p> <p>and enter deployments directory </p> <p><code>cd deployments</code></p>"},{"location":"quickstart/#step-2-system-descriptions-preparation","title":"Step 2: System descriptions preparation","text":"<p>Before the installation process takes place, system descriptions for every layer must be prepared. A system description is a YAML file, implemented as Kubernetes CRDs. Examples can be found in <code>descriptions/</code> directory. The descriptions for each layer reside in the respectively named directory: continuum, clusters, nodes. Each file MUST have the name of the corresponding hostname, followed by the .yaml or .yml suffix. For example, a machine at the node level, with hostname <code>node-1</code>, should have a description file named <code>node-1.yaml</code> under the directory <code>nodes/</code>.</p> <ul> <li>Continuum level descriptions, require one single file, that declare the continuumID and the clusters that we allow MLSysOps to manage.</li> <li>Cluster level descritptions, require a file for each cluster registered in Karmada. It contains the clusterID and a list of node hostnames, that MLSysOps is allowed to manage.</li> <li>Node level descriptions, contain the detailed information about the node resources. Example here.</li> </ul> <p>Before deploying, prepare system descriptions as Kubernetes CRDs:</p> <ul> <li>Stored in the <code>descriptions/</code> directory</li> </ul>"},{"location":"quickstart/#file-structure","title":"\ud83d\udcc1 File structure:","text":"<pre><code>descriptions/\n\u251c\u2500\u2500 continuum/\n\u2502   \u2514\u2500\u2500 &lt;continuum-hostname&gt;.yaml\n\u251c\u2500\u2500 clusters/\n\u2502   \u2514\u2500\u2500 &lt;cluster-hostname&gt;.yaml\n\u2514\u2500\u2500 nodes/\n    \u2514\u2500\u2500 &lt;node-hostname&gt;.yaml\n</code></pre> <p>Descriptions define IDs, managed components, and resource details. All files are required before installation.</p>"},{"location":"quickstart/#step-3-deploy-the-framework","title":"Step 3: Deploy the Framework","text":"<p>There are two ways to deploy the framework:</p>"},{"location":"quickstart/#option-1-automated-using-the-mlsysops-cli","title":"Option 1: Automated using the MLSysOps CLI","text":"<p>You can install the CLI in two ways:</p> <p>From TestPyPI:</p> <pre><code>pip install -i https://test.pypi.org/simple/ mlsysops-cli==0.1.9\n</code></pre> <p>From GitHub (includes deployments folder):</p> <pre><code>git clone https://github.com/marcolo-30/mlsysops-cli.git\ncd mlsysops-cli\npip install -e .\n</code></pre> <p>This exposes the <code>mls</code> command.</p> <p>Set environment variables:</p> <pre><code>export KARMADA_HOST_KUBECONFIG=&lt;path to host kubeconfig&gt;\nexport KARMADA_API_KUBECONFIG=&lt;path to api kubeconfig&gt;\nexport KARMADA_HOST_IP=&lt;host IP&gt;\n</code></pre> <p>Run deployment:</p> <pre><code>cd deployments/\nmls framework deploy-all\n</code></pre> <p>This will:</p> <ul> <li>Deploy core services (ejabberd, redis, API service)</li> <li>Register system descriptions</li> <li>Deploy all agents in correct order</li> </ul> <p>Alternative: You can also run the CLI script directly:</p> <pre><code>cd deployments\npython3 deploy.py\n</code></pre> <p>Wait for all pods to be created:</p> <pre><code>kubectl get pods -n mlsysops-framework\n</code></pre>"},{"location":"quickstart/#option-2-manual-deployment","title":"Option 2: Manual Deployment","text":"<p>Follow the order below to deploy manually if you prefer full control.</p>"},{"location":"quickstart/#management-cluster-continuum","title":"Management Cluster (Continuum)","text":"<pre><code>export KUBECONFIG=&lt;host kubeconfig&gt;\n</code></pre> <ul> <li> <p>Create namespace: <pre><code>kubectl apply -f namespace.yaml\n</code></pre></p> </li> <li> <p>Install services: <pre><code>kubectl apply -f xmpp/deployment.yaml\nkubectl apply -f api-service-deployment.yaml\nkubectl apply -f redis-stack-deployment.yaml\n</code></pre></p> </li> <li> <p>Start ML Connector: <pre><code>docker compose -f mlconnector.docker-compose.yaml up -d\n</code></pre></p> </li> <li> <p>Apply RBAC: <pre><code>kubectl apply -f mlsysops-rbac.yaml\n</code></pre></p> </li> <li> <p>Add configuration and system descriptions: <pre><code>kubectl create configmap continuum-karmadapi-config --from-file=&lt;karmada-api.kubeconfig&gt; --namespace=mlsysops-framework\nkubectl create configmap continuum-system-description --from-file=descriptions/continuum/&lt;hostname&gt;.yaml --namespace=mlsysops-framework\n</code></pre></p> </li> <li> <p>Start the Continuum Agent: <pre><code>kubectl apply -f continuum-agent-daemonset.yaml\n</code></pre></p> </li> </ul>"},{"location":"quickstart/#karmada-api-cluster-cluster-agents","title":"Karmada API Cluster (Cluster Agents)","text":"<pre><code>export KUBECONFIG=&lt;api kubeconfig&gt;\n</code></pre> <ul> <li> <p>Apply policies and namespace: <pre><code>kubectl apply -f cluster-propagation-policy.yaml\nkubectl apply -f propagation-policy.yaml\nkubectl apply -f namespace.yaml\nkubectl apply -f mlsysops-rbac.yaml\n</code></pre></p> </li> <li> <p>Add system descriptions: <pre><code>kubectl create configmap cluster-system-description --from-file=descriptions/clusters --namespace=mlsysops-framework\n</code></pre></p> </li> <li> <p>Start Cluster Agents: <pre><code>kubectl apply -f cluster-agents-daemonset.yaml\n</code></pre></p> </li> </ul>"},{"location":"quickstart/#node-agents","title":"Node Agents","text":"<ul> <li>Ensure node descriptions are in place</li> <li>Add them via ConfigMap:</li> </ul> <pre><code>kubectl create configmap node-system-descriptions --from-file=descriptions/nodes --namespace=mlsysops-framework\n</code></pre> <ul> <li>Start Node Agents:</li> </ul> <pre><code>kubectl apply -f node-agents-daemonset.yaml\n</code></pre>"},{"location":"quickstart/#step-4-deploy-a-test-application","title":"Step 4: Deploy a test application","text":"<p>We use a simple TCP Client - Server application, that send messages periodically.  The files are in <code>tests/application</code> of the repo.</p> <p>Update the <code>test_CR</code> and <code>test_MLSysOps_description</code>, with the node names of the cluster and the clusterID.</p> <p>apply the CR:</p> <p><code>kubectl apply -f tests/application/test_CR.yaml</code></p> <p>or the description via the MLS CLI:</p> <p><code>cli/mls.py apps deploy-app --path tests/application/test_MLSysOps_descirption.yaml</code></p> <p>You can watch the pods starting and be managed by the MLSysOps Framework. The client pod will be relocated every 30 seconds, with round-robin logic to every worker node.</p> <p><code>kubectl get pods -n mlsysops --context clusterID</code></p>"},{"location":"testbed/","title":"Basic Testbed","text":"<p>To install the MLSysOps framework, you will need one or more kubernetes clusters, acting as the compute nodes (cloud / edge layers) and a management node (or cluster), acting as the high-level orchestrator. In the following document we provide the necessary steps to bootstrap an example testbed.</p>"},{"location":"testbed/#quick-links","title":"Quick links","text":"<p>Prerequisites Installation Steps Setting Up Ansible Inventory Configuration k3s Installation Playbook Karmada Installation Playbook </p>"},{"location":"testbed/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure the following:</p> <ul> <li>Ansible Installed: Install Ansible on the control node.</li> <li>SSH Access: Ensure the control node has SSH access to all target nodes.</li> <li>Python 3 Installed: Ensure Python 3 is available on all nodes.</li> <li>Supported OS: The playbooks are tested on Ubuntu 22.04 LTS. Other Linux distributions may require adjustments.</li> <li>Multiple Machines: At least one machine for the management cluster (Karmada) and others for k3s clusters (master and worker nodes).</li> </ul> <p>Assuming we have bootstrapped a control node that will manage the testbed installation, we can proceed.</p>"},{"location":"testbed/#installation-steps","title":"Installation Steps","text":"<p>To set up Ansible, run the follow commands on the control node:</p> <p>1) Update System Packages to latest version.</p> <p><pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre> 2) Install essential packages that Ansible relies on:</p> <p><pre><code>sudo apt install -y python3 software-properties-common\n</code></pre> 3) Install Ansible.</p> <p><pre><code>sudo apt install -y ansible\n</code></pre> 4) After installation, confirm that Ansible is installed and working correctly by checking its version:  <pre><code>ansible --version\n</code></pre> Example output:</p> <pre><code>  ansible 2.10.8\n  config file = None\n  configured module search path = ['/home/pmavrikos/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /usr/lib/python3/dist-packages/ansible\n  executable location = /usr/bin/ansible\n  python version = 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\n</code></pre>"},{"location":"testbed/#setting-up-ansible","title":"Setting Up Ansible","text":"<p>Once Ansible is installed, you need to set up your project by cloning the repository and configuring the necessary files.</p> <p>Clone the Repository and navigate to the project directory</p> <pre><code>git clone https://github.com/mlsysops-eu/mlsysops-framework\ncd mlsysops-framework\n</code></pre>"},{"location":"testbed/#inventory-configuration","title":"Inventory Configuration","text":"<p>The inventory.yml file contains the list of target nodes where k3s and Karmada will be installed. Before running the playbooks, update this file with your specific setup.</p> <p>Understand the Structure</p> <p>The file is divided into:</p> <ul> <li>management_cluster:  The machine where Karmada will be installed (usually one node).</li> <li>cluster1: A k3s cluster with:<ul> <li>master_nodes: Control-plane nodes (you can have one or more for high availability).</li> <li>worker_nodes: Worker nodes that run workloads. </li> </ul> </li> </ul> <p>Mandatory fields to update:</p> <ul> <li><code>ansible_host</code>: Replace <code>xxxxx</code> with the IP address of each target node.</li> <li><code>ansible_user</code>: Enter the SSH username for logging into the machine</li> <li><code>ansible_ssh_private_key_file</code>:  Provide the full path to your SSH private key on the control machine. </li> <li><code>ansible_python_interpreter</code>: Ensure it points to a valid Python 3 interpreter path on each target node.</li> <li><code>k3s_cluster_name</code>: Specify a meaningful cluster name.</li> <li><code>pod_cidr</code> and <code>service_cidr</code>: Customize network ranges for pods and services (they must not overlap between clusters).</li> </ul> <p>Example Configuration</p> <pre><code>all:\n  children:\n    management_cluster: # &lt;-- In this vm will be karmada\n      hosts:\n        mls00: # &lt;-- Change with your vm name\n          ansible_host: x.x.x.x   # &lt;-- Update with the correct IP address\n          ansible_user: mlsysops\n          ansible_ssh_private_key_file: /home/xxxxxx/.ssh/id_rsa # &lt;-- Update\n          ansible_python_interpreter: /usr/bin/python3\n          k3s_cluster_name: management\n          pod_cidr: \"x.x.x.x/x\"\n          service_cidr: \"x.x.x.x/x\"\n\n    cluster1:\n      children:\n        master_nodes:\n          hosts:\n            mls01: # &lt;-- Change with your master node vm name\n              ansible_host: x.x.x.x   # &lt;-- Update with the correct IP address\n              ansible_user: mlsysops \n              ansible_ssh_private_key_file: /home/xxxxxxxx/.ssh/id_rsa  # &lt;-- Update\n              ansible_python_interpreter: /usr/bin/python3\n              k3s_cluster_name: cluster1\n              pod_cidr: \"x.x.x.x/x\"\n              service_cidr: \"x.x.x.x/x\"\n        worker_nodes:\n          hosts:\n            mls02:\n              ansible_host: x.x.x.x # &lt;-- Update with the correct IP address\n              ansible_user: mlsysops\n              ansible_ssh_private_key_file: /home/xxxxxxxxx/.ssh/id_rsa  # &lt;-- Update\n              ansible_python_interpreter: /usr/bin/python3\n              k3s_cluster_name: cluster1\n            mls03:\n              ansible_host:  x.x.x.x # &lt;-- Update with the correct IP address\n              ansible_user: mlsysops\n              ansible_ssh_private_key_file: /home/xxxxxxxxxxx/.ssh/id_rsa # &lt;-- Update\n              ansible_python_interpreter: /usr/bin/python3\n              k3s_cluster_name: cluster1\n</code></pre> <p>Verify</p> <p>After editing inventory.yml, save the file and check it for errors. You can test the inventory with:</p> <p><pre><code>ansible-inventory -i inventory.yml --list\n</code></pre> This shows all nodes Ansible will target.</p>"},{"location":"testbed/#k3s-installation-playbook","title":"k3s Installation Playbook","text":"<p>The k3s-install.yml playbook automates the deployment of a multi-node k3s cluster.</p> <ul> <li>Ensure the inventory file is updated before running the playbook.</li> <li>Execute the playbook to install k3s across all defined nodes.</li> <li>After installation, the kubeconfig file for each cluster is stored at: <pre><code>/home/&lt;ANSIBLE_USER&gt;/.kube/config\n</code></pre> on the control-plane node.</li> </ul> <p>To run k3s-install.yml playbook, use the following command: <pre><code>ansible-playbook -i inventory.yml k3s-install.yml\n</code></pre></p>"},{"location":"testbed/#karmada-installation-playbook","title":"Karmada Installation Playbook","text":"<p>The karmada-install.yml playbook sets up Karmada, a multi-cluster management system.</p> <p>To run karmada-install.yml, playbook, use the following command: <pre><code>ansible-playbook -i inventory.yml karmada-install.yml\n</code></pre></p>"},{"location":"design/","title":"Index","text":"<p>This section describes the high-level architecture of <code>MLSysOps</code>, along with the design choices and limitations.</p>"},{"location":"design/#overview","title":"Overview","text":""},{"location":"design/#execution-flow","title":"Execution flow","text":""},{"location":"design/#application-description-and-deployment","title":"Application description and deployment","text":"<p>TBC</p>"},{"location":"design/agent-configuration/","title":"Agent Configuration","text":"<p>Each agent uses a configuration file that defines its behaviour during instantiation. While agents operating at different layers of the continuum instantiate different components of the core MLSysOps framework, all agents running on nodes use the same base instance. However, since node characteristics may vary significantly, each agent can be individually configured using its corresponding configuration file.</p> <pre><code>telemetry:\n  default_metrics: \n      - \"node_load1\"\n  monitor_data_retention_time: 30\n  monitor_interval: 10s\n  managed_telemetry:\n    enabled: True\n\npolicy_plugins:\n  directory: \"policies\"\n\nmechanism_plugins:\n  directory: \"mechanisms\"\n  enabled_plugins:\n   - \"CPUFrequencyConfigurator\"\n\ncontinuum_layer: \"node\"\n\nsystem_description: 'descriptions/rpi5-1.yaml'\n\nbehaviours:\n  APIPingBehaviour:\n    enabled: False\n  Subscribe:\n    enabled: False\n</code></pre>"},{"location":"design/agents/","title":"Multi-layer Agents","text":"<p>The agent component forms the core of the MLSysOps framework. It provides essential integration logic across all layers, connecting the configuration mechanisms of the underlying system, telemetry data collected from various system entities (e.g., application, infrastructure), and system configuration policies. Figure 32 illustrates the high-level architectural structure of the agent. The component exposes two interfaces\u2014the Northbound and Southbound APIs\u2014which offer structured methods for different system users to interact with it. The Northbound API targets application and policy developers, whereas the Southbound API is primarily intended for system administrators and mechanism providers.</p> <p></p> <p>The agent follows MAPE (Monitor-Analyze-Plan-Execute) paradigm, which was proposed in 2003 [55] to manage autonomic systems given high-level objectives from the system administrators, by using the same notion for the main configuration tasks, depicted as MAPE Tasks in Figure 32. The programming language of choice is Python, and leverages SPADE Python multi-agent framework [56] to form a network of agents that can communicate through XMPP protocol and a set of defined messages, providing any necessary functionality from internal tasks that are called behaviours. To achieve seamless operation between the various sub-modules, the agent implements a set of controllers that are responsible for managing the various external and internal interactions. One important design goal of the agent was extensibility. This goal is achieved by defining simple yet powerful abstractions for two important actors interacting with the system: on one side, the policy developer, who implements the core management logic, and on the other side, the mechanism provider, who exposes the available configuration options for a subsystem. Both abstractions are integrated into the MLSysOps agent as plugin functionalities, specifically named policy and mechanism plugins. The agent's analysis, planning, and execution tasks depend on this plugin system to generate intelligent configuration decisions\u2014provided by the installed policy plugins\u2014and to apply those decisions to the underlying system via the available mechanism plugins.</p> <p></p> <p>The agent software is structured into different module types:</p> <ul> <li>Core Module \u2013 Provides foundational functionalities shared by all agent instances (continuum, cluster, and node).</li> <li>Layer-Specific Modules \u2013 Offer customized implementations specific to the roles of continuum, cluster, or node agents.</li> <li>External Interface Modules \u2013 Facilitate interactions between the agent framework and external entities. These modules include the CLI, Northbound API, ML Connector, policy and mechanism plugins.</li> </ul> <p>This modular architecture ensures consistency in core functionalities across all agents, while also supporting customization and extension for specific layers and external interactions.</p>"},{"location":"design/application-description/","title":"Application Description","text":"<p>The application owner, one of the main actors, interacts with MLSysOps by submitting the application description using the Command Line Interface (CLI) provided by the framework. The application description depicts the required deployment constraints (e.g., node-type, hardware, sensor requirements, etc.), which enable various filtering options for the continuum and cluster layers, that can decide the candidate clusters and nodes, respectively. Having as an example the registration of a given application, as shown in Figure 42, we perform a Top-Down propagation of the necessary information to each layer of the continuum. Initially, the Continuum agent creates a Kubernetes Custom Resource that is propagated to the available Kubernetes clusters. The Cluster agents follow the Kubernetes Operator pattern, so they are notified of application creation, update, or removal events. Each Cluster agent manages the components that match its cluster ID, if any. This information is provided by the Continuum agent in the application's Custom Resource. A given Cluster agent captures the application creation event, parses the description, and deploys the components based on the provided requirements. The component specifications are also sent to their host nodes, so that the Node agents can store relevant fields required for any potential reconfiguration/adaptation.</p> <p></p>"},{"location":"design/architecture/","title":"Architecture","text":"<p>MLSysOps introduces a hierarchical agent-based architecture composed of three levels: - Node Agents reside on individual nodes and expose configuration interfaces, monitor resource usage, and provide direct access to telemetry. - Cluster Agents coordinate groups of nodes, aggregate telemetry, and issue deployment decisions or adaptation instructions. - The Continuum Agent sits at the top level, interfacing with external stakeholders (via northbound APIs), receiving high-level intents and application descriptors, and coordinating decision-making across slices. Each layer operates a Monitor\u2013Analyze\u2013Plan\u2013Execute (MAPE) control loop, enabling autonomous adaptation based on local and global telemetry, system optimization targets, and ML-driven policies. Importantly, this architecture separates management logic from resource control, allowing for modular evolution and system introspection.</p> <p>The MLSysOps agents, supported by ML models, analyse, predict, and optimize resource usage patterns and overall system performance by allocating, monitoring and configuring the different resources of the underlying layers via the mechanisms that are implemented in the context of WP3 and manifested in the current deliverable. This integration is a collaborative effort that draws on the diverse expertise of project partners, each contributing unique insights and solutions to the multifaceted challenges of cloud and edge computing. This collaborative approach is complemented by an iterative development process characterized by continuous testing and feedback loops. Such a process ensures that the mechanisms developed are not only effective in their current context but are also scalable and adaptable to future technological advancements and operational needs.</p> <p></p> <p>The following figure depicts a comprehensive illustration of the MLSysOps hierarchical agent system's placement and its interactions with two other fundamental subsystems: container orchestration and telemetry. This agent hierarchy is structured in line with the orchestration architecture, and it is logically divided into three tiers. The communication among the three subsystems (agents, container orchestration, and telemetry) is facilitated through designated interfaces at each tier. Moreover, the agent system engages with the continuum level's system agents and integrates plug-in configuration policies that can use ML models at all levels. At every level, agents utilize mechanism plugins to implement commands for adjusting available configuration and execution mode options.</p> <p></p> <p>Node-level agents\u2019 interface with local telemetry systems and expose configuration knobs. Cluster-level agents coordinate resource allocation decisions across groups of nodes. At the top level, the continuum agent handles global orchestration, provides APIs to external actors, and aggregates telemetry data. ML-driven decisions can be made at every layer, using information for the respective layer. This layered approach facilitates scalability and separation of concerns while supporting collaboration across orchestration, telemetry, and ML systems. The agent infrastructure interacts through three distinct types of interfaces. The Northbound API provides access to application developers and system administrators. The Southbound API interfaces with the underlying telemetry collection and configuration mechanisms. The ML Connector allows ML models to be plugged into the framework and invoked for training, prediction, and explanation tasks. The telemetry subsystem is built upon the OpenTelemetry specification and is responsible for collecting and processing metrics, logs, and traces. These are abstracted into hierarchical telemetry streams that feed the decision logic of the agents and the ML models. Data collection happens at the node level, where individual collectors expose metrics either in raw or aggregated formats. These are processed through transformation pipelines and propagated to cluster and continuum levels for higher-level aggregation and analysis.</p> <p>Application deployment and orchestration are driven by declarative descriptions submitted by application developers and administrators. These descriptions capture the application's structure, resource requirements, and quality-of-service objectives. Deployment is handled through standard container orchestration tools, which are extended by the MLSysOps framework to support advanced placement decisions and runtime adaptation. For far-edge deployments, the framework introduces a proxy-based architecture involving embServe on constrained devices and a virtual orchestrator service running inside containerized environments. This approach allows resource-constrained devices to be seamlessly integrated into the same orchestration and telemetry flows as more capable edge and cloud nodes.</p> <p>The object storage infrastructure builds upon and extends SkyFlok, a secure and distributed storage system. In MLSysOps, this infrastructure supports adaptive reconfiguration of bucket policies based on real-time telemetry and application usage patterns. The storage system exposes telemetry data regarding latency, bandwidth, and access frequency, enabling agents and ML models to optimize redundancy and placement decisions without disrupting ongoing operations. The framework also includes specialized subsystems for anomaly detection and trust assessment. These modules analyze telemetry data to identify attacks or malfunctions and classify anomalies using ML models. Their outputs are exposed through the telemetry interface and used by higher-level agents to trigger remediation strategies or adapt orchestration plans. Trust levels for nodes are computed using a combination of identity, behaviour, and capability metrics, forming a reputation-based model that influences agent decision-making.</p> <p>ML models play a central role in enabling the autonomic operation of the framework. Each level of the agent hierarchy may employ one or more models, which are integrated via the ML Connector API. These models receive structured telemetry input and produce configuration decisions, which are interpreted and enacted by the agents. The framework supports reinforcement learning, continual learning, and federated learning scenarios. In addition, explainability mechanisms are integrated into the ML workflows to allow system administrators and application developers to understand and audit the decisions made by the models.</p> <p>MLSysOps effectively manages operations by leveraging telemetry data collected from each level, which provides essential insights. This data, combined with machine learning models, enhances the decision-making process, aligning with both the application's objectives and the system's requirements. Actions based on these decisions are cascaded and refined from the top level downwards. The final status and outcomes of these decisions are then made accessible to system Actors. The design and functionality of the telemetry system are further explaiend in Telemetry system design.</p>"},{"location":"design/controllers/","title":"Controllers","text":"<p>Controllers are responsible for coordinating all internal components of the framework, including the MAPE tasks, SPADE, Policy and Mechanism Plugins, and the Northbound and Southbound APIs.</p> <ul> <li> <p>Application Controller: Manages the lifecycle of the Analyze loop for each application submitted to the system. When a new application is submitted, a corresponding Analyze behaviour is initiated, and it is terminated when the application is removed.</p> </li> <li> <p>Policy &amp; Mechanism Plugin Controllers: Responsible for loading, initializing, and configuring policy and mechanism plugins. During runtime, these controllers provide updated information to the Application Controller, reflecting any changes in the policy API files.</p> </li> <li> <p>Agent Configuration Controller: Handles external configuration commands received from other agents or via the Northbound API, and propagates them to the appropriate internal components. It is also responsible for loading the initial configuration file during startup.</p> </li> <li> <p>Telemetry Controller: Manages the OpenTelemetry Collector for each agent, including initial deployment and runtime configuration. Since each collector operates as a pod within the cluster, the Node Agent coordinates with the Cluster Agent to request deployment and updates, as depicted in Figure 41. Additionally, this controller configures the Monitor task based on the telemetry metrics being collected.</p> </li> </ul> <p></p>"},{"location":"design/mape/","title":"MAPE Tasks","text":"<p>The primary responsibility of the agents is to manage the system\u2019s assets\u2014entities and components that can be configured and/or must be monitored. Typical assets include application components, available configuration mechanisms, and the telemetry system. The MAPE tasks continuously monitor the state of these assets, analyze their condition, determine whether a new configuration plan is required, and, if so, create and execute the plan using the mechanism plugins. The Analyze and Plan tasks invoke the logic implemented in the policy plugins, whereas the Execution task uses the mechanism plugins.</p>"},{"location":"design/mape/#monitor","title":"Monitor","text":"<p>The Monitor task runs periodically, collecting information from the environment and updating the agent's internal state. This information is sourced from the telemetry system, external mechanisms (via Southbound API mechanism plugins), and other external entities (e.g., other agents). Although there is only a single instance of the Monitor task, it is adaptive; its configuration can change at runtime based on the agent\u2019s current requirements. A fundamental configuration parameter is the frequency and type of information retrieved from the telemetry system. For example, when a new application is submitted to the system, additional telemetry metrics may need to be collected and incorporated into the internal state.</p>"},{"location":"design/mape/#analyze","title":"Analyze","text":"<p>For each distinct managed asset, a separate Analyze task thread runs periodically. This thread invokes the corresponding method of the active policy plugin (see Section 8.3.1) for the specific asset, supplying all necessary inputs, including telemetry data and relevant system information (e.g., application and system descriptions). Policy plugins may implement the analysis logic using simple heuristics or employ machine learning models, either through the MLSysOps ML Connector or via any external service. This task also includes core logic to perform basic failure checks in the event of errors arising within the policy plugins. The output of the Analyze task is a binary value (True or False), indicating whether a new configuration plan is required for the analyzed asset. If the result is True, a new Plan task is initiated.</p>"},{"location":"design/mape/#plan","title":"Plan","text":"<p>The Plan task is responsible for generating a new configuration plan and is executed once for each positive result produced by the Analyze task. The planning logic, implemented by the policy plugins, is invoked upon trigger and receives all necessary input data. The output of this task is a dictionary containing values expected by each mechanism plugin. This dictionary represents the configuration plan to be applied by the respective configuration mechanisms. The result is pushed into a queue and forwarded to the Plan Scheduler (see Section 8.1.5).</p>"},{"location":"design/mape/#execute","title":"Execute","text":"<p>This task is invoked by the Plan Scheduler (see Section 8.1.5) once for each mechanism that must be configured in a given plan. Based on the dictionary provided by the plan, the corresponding mechanism plugin is called and supplied with the relevant configuration data. The new configuration is applied using a best-effort approach, without any retry logic, and the outcome is logged into the state (see Section 8.1.6). In the event of an error, it is expected that the subsequent run of the Analyze task will detect the issue and handle it appropriately.</p>"},{"location":"design/mape/#plan-scheduler","title":"Plan Scheduler","text":"<p>Each agent supports the concurrent activation of multiple policy and mechanism plugins. As a result, different policies may generate configuration plans for the same mechanism simultaneously. This situation can lead to conflicts during plan execution, where multiple plans attempt to apply different\u2014and potentially conflicting\u2014configuration changes to the same mechanism at the same time. To handle such conflicts, the MLSysOps agent includes a Plan Scheduler module that processes the queued plans produced by Plan tasks (see Section 8.1.3) in a FIFO manner. The first plan in the queue is applied, and any subsequent plan targeting a mechanism already configured by a previous plan is discarded. The Plan Scheduler is designed to be extensible, allowing support for more advanced scheduling policies in the future. For each scheduled plan, a single Execute task (see Section 8.1.4) is launched to apply the new configuration.</p>"},{"location":"design/mape/#state","title":"State","text":"<p>This is the internal state (memory) of the agents. Each agent contains different information depending on its environment (continuum level, node type etc.). Some indicative information needed to be kept are information about the application descriptions as well as system and application telemetry. It is able to store historical snapshots of the telemetry data that has been acquired by the monitor task</p>"},{"location":"design/spade/","title":"SPADE","text":"<p>SPADE (Smart Python Development Environment) [56] is a middleware platform for multi-agent systems written in Python, leveraging the capabilities of instant messaging to manage communication between agents. It simplifies the development of intelligent agents by combining a robust platform connection mechanism with an internal message dispatcher that efficiently routes messages to various integrated behaviours. Each agent in SPADE is identified by a unique Jabber ID ( JID) and connects to an XMPP server using valid credentials. The use of XMPP not only provides persistent connections and reliable message delivery but also enables real-time presence notifications, which are essential for determining the availability of agents in a dynamic environment. The selection of SPADE as the middleware for our multi-agent system is based on several key factors. Its native Python implementation allows for seamless integration with machine learning libraries and other smart applications, ensuring that sophisticated functionalities can be embedded directly within the agents. SPADE adheres to the FIPA standards, promoting interoperability with other agent platforms such as JADE, which is crucial for systems requiring diverse communication protocols. Furthermore, its modular architecture and open-source nature foster a vibrant community for continuous improvement, supporting extensibility through plugins, and custom behaviours. This robust, flexible design not only accelerates the development cycle but also provides a reliable foundation for building complex, intelligent multi-agent systems.</p>"},{"location":"design/spade/#behaviours","title":"Behaviours","text":"<p>In SPADE, a behaviour is a modular piece of functionality that encapsulates a specific task or activity for an agent. Behaviours determine how an agent reacts to incoming messages, processes information, or interacts with its environment. They can operate in different modes:</p> <ul> <li>Cyclic and Periodic behaviours are useful for performing repetitive tasks.</li> <li>One-Shot and Time-Out behaviours can be used to perform casual tasks or initialization tasks.</li> <li>The Finite State Machine allows more complex behaviours to be built.</li> </ul> <p>This flexible structure allows us to efficiently delegate tasks without overcomplicating the agent\u2019s core logic, ensuring clean and maintainable design. Within our multi-agent system, each agent is assigned specific behaviours based on its role and the tasks it needs to perform. Each type of agent includes unique behaviours that enable it to carry out specialized tasks. For example, the Continuum Agent is responsible for interacting with both other agents and the user, incorporating behaviours for processing user requests, such as responding to a ping message to confirm aliveness, processing application and ML model descriptions, and checking the deployment status of these applications or models. And other behaviours to process agent\u2019s interactions that are also common functionalities across the agents, for example handling subscriptions and heartbeat messages.</p> <p>As previously mentioned, agents are structured across three layers: Node Agents, Cluster Agents, and a central Continuum Agent. Each type of agent is assigned a specific set of behaviours that align with its role in the system. These behaviours enable the agents to communicate, monitor health, process application logic, and adapt to runtime conditions. Below is a detailed explanation of each behaviour, followed by a description of which agent types implement them.</p> <ul> <li>Heartbeat Behaviour: This behaviour is used by node and cluster agents and is responsible for periodically sending a   signal that indicates the agent is alive. These heartbeat messages are used by higher-layer agents to maintain an   up-to-date view of active agents in the system.</li> <li>Subscribe Behaviour: Used by Node and Cluster Agents, this behaviour sends a subscription request to a higher-layer   agent. It allows the agent to join the hierarchical structure and start reporting to its parent, establishing the   control flow across layers.</li> <li>Message Receiving Behaviour: Present in all agent types, this behaviour allows an agent to handle incoming messages   from other agents. These messages may contain commands, data updates, or coordination requests. It is essential for   asynchronous interaction across the distributed system.</li> <li>Message Sending Behaviour: Also implemented by all agent types, this behaviour handles sending messages to other   agents. It enables agents to initiate communication, send results, or trigger actions elsewhere in the system.</li> <li>Management Mode Behaviour: This behaviour allows agents to switch between different decision-making strategies. It is   present across all agents and can dynamically toggle between heuristic control and machine learning-based approaches.   This flexibility allows the system to adjust its intelligence level based on runtime context or user commands.</li> <li>Policy &amp; Mechanism Plugin Management Behaviour: Implemented by all agents, this behaviour allows enabling or disabling   plugins at runtime. It supports dynamic reconfiguration of agent logic and enhances adaptability without requiring   redeployment.</li> <li>HB Receiver Behaviour: This behaviour is used in Cluster and Continuum Agents. It receives heartbeat signals sent by   lower-layer agents (e.g., Nodes), updates their status, and maintains a local registry of active agents.</li> <li>Check Inactive Agents Behaviour: This behaviour complements the heartbeat mechanism. It is used in Cluster and   Continuum Agents to scan the list of subscribed agents and identify those that have stopped sending heartbeats,   indicating failure or disconnection.</li> <li>Manage Subscription Behaviour: Implemented by Cluster and Continuum Agents, this behaviour accepts and registers   agents from a lower layer. It enables agents to expand their scope of management as new agents come online and request   to be part of the system.</li> <li>API Ping Behaviour: Exclusive to the Continuum Agent, this behaviour allows external components such as the   command-line interface to verify whether the agent is alive by sending ping messages through the North Bound API.</li> <li>Check App Deployment Behaviour: This behaviour is also unique to the Continuum Agent. It verifies that the components   of an application have been properly deployed in the framework. It ensures application consistency across the   infrastructure.</li> <li>Check ML Deployment Behaviour: Like the previous behaviour but focused on machine learning applications. It verifies   that ML services are correctly deployed and ready for operation. This behaviour also exists only in the Continuum   Agent.</li> <li>App Process Behaviour: Implemented in the Continuum Agent, this behaviour analyzes the application description and   determines how and where to deploy its components. It interprets application specifications and translates them into   deployment strategies.</li> <li>ML Process Behaviour: This behaviour, also integrated into the Continuum Agent, is responsible for managing the full   lifecycle of machine learning endpoint deployments. It handles the deployment of new ML models or services, monitors   the status of deployed models in real-time, supports redeployment in case of model updates or infrastructure changes,   and manages the deletion of endpoints when they are no longer needed. This ensures a consistent and automated approach   to maintaining ML services across the continuum infrastructure.</li> </ul> <p>Each of these behaviours is designed to work in harmony within its respective agent, ensuring that the system remains modular, scalable, and responsive to dynamic environments. As we continue to develop our framework, we can further refine each behaviour to meet the specific requirements of our agents and enhance the overall efficiency of the multi-agent system.</p> Behaviour Name Type Type of Agent Continuum Cluster Node API Ping Cyclic x Check inactive Periodic x x Check _app_deployment One Shot x Check_ml_deployment One Shot x HB Receiver Cyclic x x HeartBeat Periodic x x ML_process Cyclic x Manage Subscription Cyclic x x Management mode Cyclic x x x Message receiving Cyclic x x x Message sending One Shot x x x Process Cyclic x Subscribe Cyclic x x Policy management Cyclic x x x"},{"location":"design/spade/#messages","title":"Messages","text":"<p>SPADE agents communicate by exchanging discrete messages rather than direct method calls, embodying the \u201ccomputing as interaction\u201d paradigm of multi-agent systems. As previously mentioned, each agent is identified by a unique ID (JID) ( username@domain) and connects to an XMPP server using this ID and a password. SPADE relies on the XMPP (Extensible Messaging and Presence Protocol) as the backbone for all inter-agent communication. This means that every message an agent sends is transmitted as an XMPP message stanza through the server to the target agent. By using XMPP\u2019s standard messaging infrastructure, SPADE ensures that agents can reliably send and receive messages in real time, even across different hosts or network environments. In essence, the XMPP server mediates the exchange, routing each message to the intended recipient agent (identified by its JID) whether the agents reside on the same machine or are distributed over the Internet. This decoupled, server-mediated communication model provides a robust and standardized way for agents to interact, leveraging XMPP\u2019s features for authentication, presence, and security (e.g. encryption) built into the protocol. For this XMPP server, the open-source ejabberd service was selected due to its superior scalability, reliability, performance, security, ease of integration with SPADE, and strong community support. The configuration of the service is made on the config file providing a domain and each agent can register into it by using a jabber id and a password. Once the agent is registered it is ready to start exchanging messages with other registered agents.</p>"},{"location":"design/spade/#message-dispatching-and-templates","title":"Message Dispatching and Templates","text":"<p>Within each SPADE agent, an internal message dispatcher handles incoming and outgoing messages. This dispatcher functions like a mail sorter: when a message arrives for the agent, the dispatcher automatically places it into the proper \u201cmailbox\u201d for handling, and when the agent sends a message, the dispatcher injects it into the XMPP communication stream. The key to this routing is the use of message templates. Each behaviour (task) running within an agent can be associated with a message template that defines the criteria for messages it is interested in. A template can specify fields such as the sender\u2019s JID, the message\u2019s content or subject, thread/conversation ID, or metadata like performative and ontology. When an agent receives a message, the dispatcher compares the message against the templates of all active behaviours and delivers the message to the behaviour whose template it matches. In this way, templates act as filters to ensure each behaviour only processes relevant messages. For example, a template might match messages with a particular sender and a specific performative type, so that only messages from that sender with that communicative intent will trigger the associated behaviour. Messages that meet the template conditions are queued in the target behaviour\u2019s mailbox, where the behaviour can retrieve them asynchronously. This template-based filtering and routing mechanism allows multiple behaviours to run concurrently in an agent without interfering with each other, as each behaviour will only pick up the messages meant for it. It provides a structured approach to message handling, simplifying the development of complex interactions (such as protocol exchanges) by separating them into different behaviour handlers listening for different message patterns.</p>"},{"location":"design/spade/#fipa-standards-for-structured-communication","title":"FIPA Standards for Structured Communication","text":"<p>SPADE\u2019s messaging model also draws from established standards to ensure that communications are well-structured and interoperable. In particular, SPADE supports message formats inspired by the FIPA (Foundation for Intelligent Physical Agents) Agent Communication Language standards. FIPA defines a set of message fields and interaction protocols intended to promote clear semantics and compatibility among agents. In SPADE, each message\u2019s metadata can include standard FIPA-ACL fields like the performative (which describes the intent of the message, such as \u201cinform\u201d or \u201crequest\u201d), the ontology (which defines the domain of discourse or vocabulary of the message content), and the language (the format of the message content). By allowing these fields in the message structure, SPADE ensures that every message carries not just raw data but also contextual information about how to interpret that data. Adhering to FIPA communication standards means that SPADE agents follow a common protocol syntax and semantics, which in principle makes it easier for them to interact with agents from other FIPA-compliant platforms. In other words, the use of well-defined performatives and message fields imposes a consistent structure on messages, reducing ambiguity and enhancing interoperability. This standards-based approach to message handling helps achieve a level of consistency in agent communication, so that the intent and context of messages are understood in a uniform way across different agents and systems. Ultimately, SPADE\u2019s alignment with FIPA standards reinforces structured agent interactions and lays the groundwork for integration with the broader multi-agent ecosystem where such standards are followed.</p>"},{"location":"design/spade/#interactions-coordination","title":"Interactions \u2013 Coordination","text":"<p>Agents rely on behavioural logic and message exchanges to interact and coordinate tasks across the layers of the continuum. Below is an example illustrating how subscription and heartbeat mechanisms are implemented using agent behaviours to facilitate this interaction.</p>"},{"location":"design/spade/#subscription-and-heartbeat-messages","title":"Subscription and Heartbeat Messages","text":"<p>Subscription and heartbeat messages are essential processes used to register available nodes within the continuum and to maintain up-to-date information about the status of nodes and agents across the entire system.</p> <p></p> <p>In the subscription process, a lower-layer agent sends a subscription request using the subscribe performative to an upper-layer agent. Since the behaviour is cyclic, the lower-layer agent continues to send subscription requests until it receives a subscription accepted message from the upper-layer agent. Once the acknowledgment is received, the agent stops the cyclic subscription behaviour and initiates the periodic heartbeat behaviour.</p> <p>During the heartbeat phase, the lower-layer agent periodically sends heartbeat (HB) messages using the inform performative. The upper-layer agent, which runs a heartbeat receiver behaviour, constantly listens for these messages and updates its records based on the latest HB information.</p> <p>As shown in Figure above, the interaction between agents through message exchanges and behaviour logic enables the coordinated execution of various tasks across the continuum. In this example, the focus is on the subscription and heartbeat process. These coordinated mechanisms allow agents to collaborate and support broader functionalities within the framework, </p>"},{"location":"design/system-description/","title":"System Description","text":"<p>The infrastructure descriptions must be provided during the agent installation process. Taking the example of a Node description registration, it is propagated to the framework using a Bottom-Up approach, in contrast to the application registration solution (Top-Down), as shown in Figure 43. To this end, we follow the usual node registration protocols, e.g., node registration to a Kubernetes cluster. In our case, the Node agent sends the respective description to the Cluster agent, which transforms it into a Custom Resource and applies it to Kubernetes. In addition, the Cluster agent updates the Cluster formal description (also defined as a Custom Resource) with high-level information that can be used by the Continuum agent in order to perform filtering based on the available sensors, accelerators, and node types (e.g., to meet any relevant application deployment constraints). Finally, the Cluster agent notifies the Continuum agent, via the agent protocol, so that the latter can update its Cluster-related structures.</p> <p></p>"},{"location":"design/telemetry/","title":"Telemetry System","text":"<p>The telemetry plane of MLSysOps collects the data necessary from all layers to drive the configuration decisions, potentially made using ML inference and continual training, with appropriate aggregation and abstraction towards the higher layers of the hierarchy. This section will give the technical insights of the MLSysOps telemetry system that supports the collection of performance metrics across MLSysOps resource management layers.</p>"},{"location":"design/telemetry/#opentelemetry-specification","title":"OpenTelemetry Specification","text":"<p>The MLSysOps framework operates on different layers of the cloud-edge-far-edge continuum and manages highly heterogeneous systems and applications while simultaneously providing appropriate observability user interfaces, as illustrated in Figure 5. Given the diversity of tools and vendors involved, a vendor- and tool-agnostic protocol for collecting and transmitting telemetry data is essential.</p> <p></p> <p>OpenTelemetry, a well-defined open-source system, provides the foundation for MLSysOps observability capabilities. OpenTelemetry is an all-around observability framework, that handles all necessary signals (categories of telemetry data), such as traces, metrics, and logs. MLSysOps supports and uses all three signal categories. The basic signal that is exposed to the framework\u2019s users are metrics, whereas logs and traces are used for debugging and profiling purposes.</p> <p>OpenTelemetry offers the API as well as the software components that implement various telemetry functionalities, in the form of a Software Development Kit (SDK).</p> <p></p> <p>The central software component is the OpenTelemetry Collector (OTEL Collector), a vendor-agnostic implementation of the telemetry pipeline (as presented in Figure 6) that consists of three stages: i) receive, ii) process, and iii) export. This component is versatile and flexible, being able to receive telemetry data in multiple formats, process them in different ways, and export them to other systems. The OTEL Collectors can operate in two different modes: i) Agent and ii) Gateway mode. The main difference is that in Gateway mode, the collector receives telemetry data from other collectors (that, in turn, operate in Agent or Gateway mode). This makes the gateway a centralized aggregator for any underlying OTEL collectors.</p> <p>The OTEL Collector [4] baseline implementation offers the minimum required functionality, which is a set of receivers and exporters that communicate using data conforming to the OpenTelemetry Data Specification, using either HTTP or gRPC protocols, as well as specific processing capabilities, like batching and memory control. The OpenTelemetry collector repository [4] includes multiple plugin implementations for all three stages of the telemetry pipeline, like Prometheus receivers &amp; exporters, as well as a routing processor that can create different telemetry streams in the OTEL Collector pipeline. In MLSysOps, we fully leverage the available plugins to achieve the desired functionality for the OTEL Collectors at each level of the MLSysOps hierarchy: node, cluster, and continuum.</p> <p>The OpenTelemetry specification defines a way to collect and transfer telemetry data without making any assumptions about the storage of the data. MLSysOps follows the same paradigm and does not make any assumptions for this matter, although, for the development of the framework, we use the Mimir metrics database [5], which is an open-source software that is suitable for the needs of our telemetry system. Mimir is deployed at the highest level (continuum), storing telemetry data and offering an API for flexible telemetry data querying through its proprietary PromQL interface,. PromQL [6] is quite versatile and powerful for time-series data, allowing different clients (such as ML models) to easily consume the data they need, using further aggregation and transformation functions. On top of the Mimir database, MLSysOps uses Grafana [7] for data visualization, Loki [8] for the logs messages database, and Grafana Tempo [9] for trace storage. All these components belong to the same ecosystem and work seamlessly with each other without a need for further configuration. We leverage the PromQL for the MLSysOps Telemetry API at the higher levels, implementing the necessary functionality for providing telemetry data to other entities of the system, and the Prometheus metric format for providing telemetry data at the same level.</p> <p>The deployment of the OTEL Collectors in each node is performed using the appropriate containers, and the orchestration is done through the same orchestrator as the ones used for application components. This simplifies the management and the connectivity between the application components and the OTEL Collectors. The deployment configuration is done transparently and automatically by the MLSysOps framework, which is responsible for the initial deployment and configuration of the OTEL Collector pods on every node and layer, as well as the dynamic reconfiguration of the collectors at runtime.</p>"},{"location":"design/telemetry/#node-level-telemetry","title":"Node Level Telemetry","text":"<p>On each node, the OTEL Collector operates in Agent mode. As Figure 7 illustrates, it receives data from any entity that needs to emit telemetry data into the telemetry system. This is done either through the available interfaces, as they are discussed in Section 2.7, or through the appropriate configuration of the OTEL Collector enabling the desired receiver. It then periodically pushes telemetry data to the OTEL Collector, which operates in Gateway mode at the cluster level. OTEL Collectors in each node can process the data in batches, keeping the overhead low. For instance, for an application component sending telemetry data at a high rate, the collector agent can store the data in memory ( perhaps even process them to perform filtering and aggregation; see next) and then forward it to the gateway at a lower rate. It is also possible to apply transformations and aggregations to the raw data before forwarding them to the gateway collector. Note that the OTEL Collector at the node level can route the raw and the transformed telemetry data to different exporters. The raw data exporting route, provides an endpoint that can be queried locally.</p> <p></p>"},{"location":"design/telemetry/#cluster-level-telemetry","title":"Cluster Level Telemetry","text":"<p>At the cluster level, different components need to be monitored. The telemetry data in this layer must describe the status of the cluster rather than of a specific node. The main source of information for this level is the orchestration manager. There is, therefore, a dedicated OTEL Collector configured to collect metrics from the orchestrator.</p> <p></p>"},{"location":"design/telemetry/#continuum-level-telemetry","title":"Continuum Level Telemetry","text":"<p>At the highest level of the MLSysOps framework, telemetry data is used not only for configuration decisions but also for informational and debugging purposes. This layer also includes components for telemetry data storage and visualization.</p> <p></p>"},{"location":"design/plugins/mechanism_plugins/","title":"Mechanism Plugins","text":"<p>The MLSysOps framework does not impose assumptions about the underlying system architecture, recognizing that real-world infrastructures often consist of heterogeneous systems with varying adaptation capabilities and operational requirements. Different types of nodes offer different configuration options, and nodes operating at higher levels of the continuum (e.g., cluster or continuum nodes) have distinct configuration needs. To ensure seamless integration\u2014especially with the policy plugins\u2014MLSysOps defines a standardized plugin interface for system administrators and mechanism providers, known as mechanism plugins.</p> <p>To develop a mechanism plugin, a Python script must be provided, implementing three methods: (i) <code>apply</code>, (ii) <code>get_status</code>, and (iii) <code>get_options</code>. The plugin module may use any required libraries, and it is assumed that any necessary packages are pre-installed along with the agent.</p> <p>The methods are defined as follows [footnote: examples refer to CPU frequency control on a node]:</p> <p>apply: This is the primary method invoked by an Execute task. It accepts a single argument, <code>command</code>, which is a dictionary whose structure is defined and documented by the mechanism plugin. This dictionary is produced by the <code>plan</code> method of a policy plugin. The policy developer must be familiar with the available mechanism plugins in the system and the expected format of the <code>command</code> argument. Figure X shows an example of a CPU configuration plugin that utilizes supporting libraries, as described in Section 3.2. The expected dictionary structure is documented in the method\u2019s comment section, followed by the call to the underlying library to apply the specified configuration.</p> <pre><code>def apply(command: dict[str, any]):\n    \"\"\"\n    Applies the given CPU frequency settings based on the provided parameters.\n\n    This method modifies the CPU's frequency settings by either applying the changes across\n    all CPUs or targeting a specific CPU. The modifications set a new minimum and maximum\n    frequency based on the input values.\n\n    Args:\n        command (dict):\n         {\n            \"command\": \"reset\" | \"set\",\n            \"cpu\": \"all\" | \"0,1,2...\",\n            \"frequency\" : \"min\" | \"max\" | \"1000000 Hz\"\n        }\n    \"\"\"\n    # The rest of the code ommited \n    cpufreq.set_frequencies(command['frequency'])\n    # .....\n</code></pre> <p>Figure X. CPU Frequency configuration mechanism plugin (apply method).</p> <p>get_status: This method must return any available relevant status of the underlying mechanism. Figure X, shows the CPU frequency configuration mechanism plugin, the method return the current frequencies of all the CPU cores. This is used by the Monitoring and Execute task, to observe the general status of the mechanism. The mechanism provider may not implement this method, and opt-in to push the state into the telemetry stream.</p> <pre><code>def get_status():\n    \"\"\"\n    Retrieves the current CPU frequencies.\n\n    Returns:\n        list: A list of integers representing the current frequencies of the CPU cores. The\n        frequencies are usually measured in MHz.\n    \"\"\"\n    return get_cpu_current_frequencies()\n</code></pre> <p>Figure X. CPU Frequency mechanism status method.</p> <p>get_options: It returns the available configuration options for the mechanism that is handled by the plugin. In the example in Figure X, it returns the available CPU frequency steps, that can be used as values in the frequency key of the command dictionary of the apply method. This is meant to be used in a development environment, where MLSysOps framework provides suitable logging methods.</p> <pre><code>def get_options():\n    \"\"\"\n    Retrieves the list of CPU available frequencies.\n\n    Returns:\n        list: A list of frequencies supported by the CPU.\n    \"\"\"\n    return get_cpu_available_frequencies()\n</code></pre> <p>Figure X. CPU Frequency mechanism status method.</p> <p>The relationship and interaction between the policy and mechanism plugins are demonstrated in section 2.4.4.</p>"},{"location":"design/plugins/plugin_system/","title":"MLSysOps Plugin System","text":"<p>The MLSysOps framework provides a structured plugin mechanism that enables a modular approach to integrating configuration decision logic with arbitrary mechanisms. Plugins are categorized into two types: core and custom. Core plugins are always enabled and are used by MLSysOps to perform essential management functions. In contrast, custom plugins can be configured and activated either during installation or at runtime, allowing flexibility and extensibility based on system requirements.</p> <p>Core plugins provide essential policies and mechanisms. The following table briefly describes the initial plugins that have been developed, up to the time of writing of this document</p>"},{"location":"design/plugins/plugin_system/#execution-flow","title":"Execution Flow","text":"<p>Figure X illustrates the execution flow of the MAPE tasks and the integration of both policy and mechanism plugins. The Monitor task runs periodically at all times, regardless of whether an application has been submitted, collecting telemetry data and updating the local state. When a new application is submitted to the system, a separate Analyze task thread is launched, which uses the analyze method of the corresponding policy plugin. Based on the result, the analysis session either terminates or triggers a background Plan task.</p> <p>The Plan task then invokes the policy plugin\u2019s plan method to generate a new configuration plan. If a valid plan is produced, it is pushed into a FIFO queue. The Plan Scheduler periodically processes the plans in the queue and initiates an Execute task for each mechanism included in the output of the policy plugin\u2019s plan method. The Plan Scheduler enforces a constraint that prevents two different plans from applying configuration changes to the same mechanism within the same scheduling cycle.</p> <p>Finally, the Execute task for each mechanism calls the apply method of the corresponding mechanism plugin. The results of the configuration are made visible to the next execution of the Analyze task, either via direct status retrieval or through the telemetry stream.</p> <p></p>"},{"location":"design/plugins/plugin_system/#plugin-inventory","title":"Plugin Inventory","text":"Policy Name Functionality Type Continuum Layer Static Placed Components It places specific components on a node. Core Cluster Dynamic Placed Components Places application components on a node, based on application description criteria. Core Cluster Cluster Component Placement Places application components in a cluster, based on application description criteria. Core Cluster Smart Agriculture Drone Management Decides on the usage of a drone in a field. Custom Cluster Smart Agriculture Application Prediction It predicts the application performance of smart agriculture. Does not produce any plan. Custom Node Smart City Noise Prediction Predicts the existence of people, using sound sensors. Does not produce any plan. Custom Node Smart City Cluster Management Based on the node-level prediction metrics, it configures the application deployment. Custom Cluster CC Storage Gateway Placement Decindes on the CC storage gateway container. Custom Cluster Mechanism Plugin Name Functionality Type Continuum Layer Fluidity It manages application component placement in a Kubernetes cluster Core Cluster ContinnumComponent Placement It places the components to specific clusters. Core Continuum CPUFrequency It configures the CPU frequency for the supported architectures. Core Node NVIDAGPUFrequency It configures the GPU Frequency for the supported architectures. Core Node FPGAConfigurator It configures the active bitrstream of Xilinx MPSoC FPGA. Core Node vAccelPlugin Configures the vAccel plugin used by a component. Core Node CCStorage Configures the storage policy of a storage gateway. Custom Cluster NetworkRedirection Configures the network interfaces that are used by application components. Custom Cluster ChangePodSpec It configures the pod specifications for specific values. Core Node, Cluster"},{"location":"design/plugins/policy_plugins/","title":"Policy Plugins","text":"<p>Policy plugins are the components responsible for determining if a new adaptation is required and generating new configuration plans. They follow the MAPE paradigm, specifically implementing the Analyze and Plan tasks. A policy plugin is implemented as a Python module, which may import and use any external libraries, and must define three specific functions: (i) initialize, (ii) analyze (async), and (iii) plan (async). Each method requires specific arguments and must return defined outputs. Each method accepts a common argument, context, which can be used to maintain state between different method calls, as well as across consecutive invocations of the policy plugin. Policy plugins can be bound to a node or multiple applications; however, they need to decide on at least one mechanism.</p> <p>Important notes: * Each policy filename must start with the prefix <code>policy-</code>.</p> <ul> <li>The role of <code>context</code> structure is to be used across different policy function executions and store useful data. It is up to the policy developer to configure this as needed. One  can also use <code>initialize()</code>. In the following examples we use a dictionary as context.</li> </ul> <p>The methods are described as follows:</p> <p>initialize: This method contains the initialization configuration required by the agent. It is called during the plugin loading phase, and it must return the context dictionary with specific values. An example is shown in Figure 35, where the policy declares the telemetry configuration it requires, the mechanisms it will analyze and manage, any custom Python packages needed by the script, and any additional agent configuration parameters. An important parameter is to declare if this policy will make use of machine learning - this enables the usage of the ML Connector interface and accordingly configures the mechanism that enables/disables machine learning usage in the framework.</p> <pre><code>def initialize():\n    context = {\n        # The required values\n        \"telemetry\": {\n            \"metrics\": [\"node_load1\"],\n            \"system_scrape_internval\": \"1s\"\n        },\n        \"mechanisms\": [\n            \"CPUFrequency\"\n        ],\n        \"packages\": [\n            ## Any possible required Python packages needed\n        ],\n        \"configuration\": {\n            # Agent configuration\n            \"analyze_interval\": \"4s\"\n        },\n        \"machine_learning_usage\": false,\n        # ... any other fields that the policy needs\n    }\n\n    return context\n</code></pre> <p>analyze: The purpose of this method is to analyze the current state of the system and the target application, and determine whether a new configuration plan might be required. In the example shown in Figure X, the analyze function compares the current telemetry data for the application\u2014retrieved using the application description\u2014with the target value specified by the application. If the current value exceeds the defined threshold (target), the method concludes that a new plan is needed. In this example, it is assumed that the monitored application metric should remain below the specified target. The analyze method can also make use of the ML Connector interface, to make use of machine learning models deployed from that service.</p> <pre><code>async def analyze(context, application_descriptions, system_description, mechanisms, telemetry, ml_connector):\n    application = application_descriptions[0]\n\n    # policy that checks if application target is achieved\n    if telemetry['data']['application_metric'] &gt; application['targets']['application_metric']:\n        return True, context  # It did not achieve the target - a new plan is needed      \n\n    return False, context\n</code></pre> <p>Figure X. Analyze method example</p> <p>plan: This method decides if a new plan is needed, and if it is positive, generates a new configuration plan based on all available information in the system, including application and system descriptions, telemetry data. It may also leverage either internal logic/libraries or the ML Connector interface to invoke machine learning models. In the example shown in Figure X, the plan method creates a new configuration for the CPU frequency of the node on which it runs. If the application target is not met, the method sets the CPU to the maximum available frequency; otherwise, it sets it to the minimum. The configuration values used in the plan are predefined and known to the policy developer, based on the specifications of the corresponding mechanism plugin (see Section 2.4.2 for examples).</p> <pre><code>async def plan(context, application_descriptions, system_description, mechanisms, telemetry, ml_connector):\n    application = application_descriptions[0]\n\n    if telemetry['data']['application_metric'] &gt; application['targets']['application_metric']:\n        cpu_frequency_command = {\n            \"command\": \"set\",\n            \"cpu\": \"all\",\n            \"frequency\": \"max\"\n        }\n    else:\n        cpu_frequency_command = {\n            \"command\": \"set\",\n            \"cpu\": \"all\",\n            \"frequency\": \"min\"\n        }\n\n    new_plan = {\n        \"CPUFrequency\": cpu_frequency_command\n    }\n\n    return new_plan, context\n</code></pre> <p>Figure X. Plan method example</p> <p>For both the <code>analyze</code> and <code>plan</code> methods, the arguments are as follows: - context: Custom user-defined structure. - application_descriptions: A list of dictionaries containing values from the submitted   applications in the system (see Section X). - system_description: A dictionary containing system information provided by the system administrator (see Section   X). - mechanisms: A list of the available mechanisms that the policy can exploit. E.g., fluidity for app deployment, adaptation  and monitoring at the cluster-level, and CPU-freq at the node level. - telemetry: A dictionary containing telemetry data from both the system and the applications. - ml_connector: An object handler providing access to the ML Connector service endpoint within the slice. This   argument is empty if the ML Connector service is not available [*][see documentation].</p> <p>As described in Section 2.1, the above plugin methods are invoked and executed within the respective Analyze and Plan tasks. The Plan Scheduler ensures that any conflicts between different policy-generated plans are resolved and forwards them to the Execute tasks, which utilize the mechanism plugins to apply the configuration to the system. The declaration of machine learning model usage for each plugin enables MLSysOps to track where and when machine learning mechanisms are employed, monitor their performance, and disable plugins that utilize AI tools if requested. The plug-and-play support further allows for the dynamic modification of configuration logic, enabling agents to adapt to varying operational scenarios.</p> <p>Also refer to policy examples for indicative policy implementations.</p>"},{"location":"developer-guide/","title":"Index","text":"<p>In this section we provide useful information regarding the development of the <code>MLSysOps framework</code>. In particular, the section contains information about setting up a dev environment, checking performance traces and more.</p> <p>We kindly ask you to check the contributing page before submitting any pull requests, or opening new issues.</p>"},{"location":"developer-guide/Code-of-Conduct/","title":"Code of Conduct","text":"<p>All maintainers and community members of the <code>MLSysOps framework</code> must abide by the CNCF Code of Conduct.</p>"},{"location":"developer-guide/contribute/","title":"Contributing","text":"<p>The MLSysOps framework is an open-source project licensed under the Apache License 2.0. We welcome anyone who would be interested in contributing to <code>MLSysOps framework</code>. As a first step, please take a look at the following document. The current document provides a high level overview of <code>MLSysOps framework</code>'s code structure, along with a few guidelines regarding contributions to the project.</p>"},{"location":"developer-guide/contribute/#table-of-contents","title":"Table of contents:","text":"<ol> <li>Code organization</li> <li>How to contribute</li> <li>Opening an issue</li> <li>Requesting new features</li> <li>Submitting a PR</li> <li>Style guide</li> <li>Contact</li> </ol>"},{"location":"developer-guide/contribute/#code-organization","title":"Code organization","text":"<p>The MLSysOps framework is structured as follows:</p> <p>TBC</p>"},{"location":"developer-guide/contribute/#how-to-contribute","title":"How to contribute","text":"<p>There are plenty of ways to contribute to an open source project, even without changing or touching the code. Therefore, anyone who is interested in this project is very welcome to contribute in one of the following ways:</p> <ol> <li>Using <code>MLSysOps framework</code>. Try it out yourself and let us know your experience. Did everything work well? Were the instructions clear?</li> <li>Improve or suggest changes to the documentation of the project. Documentation is a very important part of every project, hence any ideas on how to make it more clear are more than welcome.</li> <li>Request new features. Any proposals for improving existing features or adding new ones are very welcome.</li> <li>Find a bug and report it. Bugs are everywhere and some are hidden very well. As a result, if you find a bug, we would really appreciate it if you reported it to the maintainers.</li> <li>Make changes to the code. Improve the code, add new functionality, and make <code>MLSysOps framework</code> even more useful.</li> </ol>"},{"location":"developer-guide/contribute/#opening-an-issue","title":"Opening an issue","text":"<p>We use Github issues to track bugs and requests for new features. Anyone is welcome to open a new issue, either to report a bug or to request a new feature.</p>"},{"location":"developer-guide/contribute/#reporting-bugs","title":"Reporting bugs","text":"<p>To report a bug or misbehavior in <code>MLSysOps framework</code>, a user can open a new issue explaining the problem. For the time being, we do not have a strict template for reporting issues, however, it is important that enough information is provided for the problem to be easily identified and resolved. To that end, when opening a new issue regarding a bug, we kindly ask you to:</p> <ul> <li>Mark the issue with the bug label</li> <li>Provide the following information:</li> <li>A short description of the bug.</li> <li>The respective logs both from the output and <code>containerd</code>.</li> <li>Framework's version manifest (either the commit hash or the version manifest file).</li> <li>The execution environment (CPU architecture, VMM etc.).</li> <li> <p>Any particular steps to reproduce the issue.</p> </li> <li> <p>Keep an eye on the issue for possible questions from the maintainers.</p> </li> </ul> <p>The following template may serve as a useful guide:</p> <pre><code>## Description\nAn explanation of the issue\n\n## System info\n\n- Version:\n- Arch:\n- VMM:\n- ...\n\n## Steps to reproduce\nA list of steps that can reproduce the issue.\n</code></pre>"},{"location":"developer-guide/contribute/#requesting-new-features","title":"Requesting new features","text":"<p>We are very happy to hear about features that you would like to see in <code>MLSysOps framework</code>. One way to communicate such a request is using Github issues. For the time being, we do not use a strict template for requesting new features, however, we kindly ask you to mark the issue with the enhancement label and provide a description of the feature.</p>"},{"location":"developer-guide/contribute/#submitting-a-pr","title":"Submitting a PR","text":"<p>Everyone should feel free to submit a change or an addition to the codebase of <code>MLSysOps framework</code>. Currently, we use Github's Pull Requests (PRs) to submit changes to <code>MLSysOps framework</code>'s codebase. Before creating a new PR, please follow the guidelines below:</p> <ul> <li>Make sure that the changes do not break the building process of <code>MLSysOps framework</code>.</li> <li>Make sure that all tests run successfully.</li> <li>Make sure to sign-off your commits.</li> <li>Provide meaningful commit messages, briefly describing the changes.</li> <li>Provide a meaningful PR message.</li> </ul> <p>As soon as a new PR is created the following workflow will take place:</p> <ol> <li>The creator of the PR should invoke the tests by adding the <code>ok-to-test</code> label.</li> <li>If the tests pass, request that one or more <code>MLSysOps framework</code>'s maintainers review the PR.</li> <li>The reviewers submit their review.</li> <li>The author of the PR should address all the comments from the reviewers.</li> <li>As soon as a reviewer approves the PR, an action will add the appropriate git trailers in the PR's commits.</li> <li>The reviewer who accepted the changes will merge them.</li> </ol>"},{"location":"developer-guide/contribute/#labels-for-the-ci","title":"Labels for the CI","text":"<p>We use github workflows to invoke some tests when a new PR opens for <code>MLSysOps framework</code>. In particular, we perform the following workflows tests:</p> <ul> <li>Commit message linting: Please check the git commit message style below for more info.</li> <li>Spell checking: since the <code>MLSysOps framework</code> repository contains its documentation too.</li> <li>License check</li> <li>Code linting</li> <li>Building artifacts for amd64 and aarch64.</li> <li>Unit tests</li> <li>End-to-end tests</li> </ul> <p>For better control over the tests and workflows that run in a PR, we define three PR labels:</p> <ul> <li><code>ok-to-test</code>: Runs a full CI workflow, meaning all lint tests (commit   message, spellcheck, license), Code linting, building for x86 and aarch64,   unit tests, and finally, end-to-end tests.</li> <li><code>skip-build</code>: Skips the building workflows along with unit test and end-to end tests, while still running all linters. This is useful when   the PR is related to docs because it can help catch spelling errors, etc. In   addition, if the changes are not related to the codebase, running the   end-to-end tests is not required and saves some time.</li> <li><code>skip-lint</code>: Skips the linting phase. This is particularly useful on draft   PRs, when we want to just test the functionality of the code (either a bug   fix, or a new feature) and defer the cleanup/polishing of commits, code, and   docs until the PR will be ready for review.</li> </ul> <p>Note: Both <code>skip-build</code> and <code>skip-lint</code> assume that the <code>ok-to-test</code> label is added.</p>"},{"location":"developer-guide/contribute/#style-guide","title":"Style guide","text":""},{"location":"developer-guide/contribute/#git-commit-messages","title":"Git commit messages","text":"<p>Please follow the guidelines below for your commit messages:</p> <ul> <li>Limit the first line to 72 characters or less.</li> <li>Limit all other lines to 80 characters.</li> <li>Follow the Conventional Commits   specification and, specifically, format the header as <code>&lt;type&gt;[optional scope]: &lt;description&gt;</code>, where <code>description</code> must not end with a full stop and <code>type</code>   can be one of:</li> <li>feat: A new feature</li> <li>fix: A bug fix</li> <li>docs: Documentation only changes</li> <li>style: Changes that do not affect the meaning of the code (white-space,     formatting, missing semi-colons, etc)</li> <li>refactor: A code change that neither fixes a bug nor adds a feature</li> <li><code>perf</code>: A code change that improves performance</li> <li>test: Adding missing tests</li> <li>build: Changes that affect the build system or external dependencies     (example scopes: <code>gulp</code>, <code>broccoli</code>, <code>npm</code>)</li> <li>ci: Changes to our CI configuration files and scripts (example scopes:     <code>Travis</code>, <code>Circle</code>, <code>BrowserStack</code>, <code>SauceLabs</code>)</li> <li>chore: Other changes that don't modify source code or test files</li> <li> <p>revert: Reverts a previous commit</p> </li> <li> <p>In case the PR is associated with an issue, please refer to it, using the git trailer <code>Fixes: #&lt;issue number&gt;</code>, i.e. <code>Fixes: #30</code>.</p> </li> <li>Always sign-off your commit message.</li> </ul> <p>Since the <code>MLSysOps framework</code> comprises code written in various programming languages, we use the following styles for each:</p>"},{"location":"developer-guide/contribute/#golang-code-style","title":"Golang code style","text":"<p>We follow <code>gofmt</code>'s formatting rules. Therefore, we ask all contributors to do the same. Go provides the <code>gofmt</code> tool, which can be used for formatting your code.</p>"},{"location":"developer-guide/contribute/#python","title":"Python","text":"<p>TBC</p>"},{"location":"developer-guide/contribute/#c","title":"C","text":"<p>TBC</p>"},{"location":"developer-guide/contribute/#contact","title":"Contact","text":"<p>Feel free to contact any of the maintainers or using one of the below email addresses:</p> <ul> <li>info@mlsysops.eu</li> </ul>"},{"location":"developer-guide/development/","title":"Setup a Dev environment","text":"<p>Most of the steps are covered in the installation document. Please refer to it for:</p> <p>TBC</p> <p>The next step is to clone and build the <code>MLSysOps framework</code>:</p> <pre><code>git clone https://github.com/mlsysops-eu/mlsysops-framework\ncd mlsysops-framework\n</code></pre>"},{"location":"developer-guide/maintainers/","title":"The current maintainers of the <code>MLSysOps framework</code>","text":"Name GitHub Username Email Responsibility Anastassios Nanos @ananos ananos@nubificus.co.uk Core Maintainer Panagiotis Mavrikos @panosmaurikos pmavrikos@nubificus.co.uk Core Maintainer Alexandros Patras @alexanpatr patras@uth.gr Core Maintainer Foivos Pournaropoulos @Foivos-Pournaropoulos spournar@uth.gr Core Maintainer Marco Loaiza @marcolo-30 Core Maintainer ..."},{"location":"mlconnector/Installation/","title":"Installation","text":""},{"location":"mlconnector/Installation/#mlconnector-installation","title":"MLConnector Installation","text":"<p>This guide will walk you through setting up and running the MLConnector  using Docker.</p>"},{"location":"mlconnector/Installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed on your system:</p> <ul> <li>Docker: Install Docker Engine and Docker Compose from Docker\u2019s official website.</li> </ul>"},{"location":"mlconnector/Installation/#environment-variables","title":"Environment Variables","text":"<p>The MLConnector relies on several external components. Define the following environment variables in your shell or an <code>.env</code> file:</p>"},{"location":"mlconnector/Installation/#1-docker-registry","title":"1. Docker Registry","text":"<p>The MLConnector dynamically creates and stores docker images for inference applications used within MYLSysOps. As such, it needs to to be able to communicate to a registry weather public, or private. This application was tested with docker registry. For further information on docker registry check.</p> <ul> <li><code>DOCKER_REGISTRY_ENDPOINT</code>: Your Docker registry endpoint</li> <li><code>DOCKER_USERNAME</code>: Your Docker registry username</li> <li><code>DOCKER_PASSWORD</code>: Your Docker registry password</li> </ul>"},{"location":"mlconnector/Installation/#2-aws-file-storage","title":"2. AWS (File Storage)","text":"<p>The MLConnector uses an external storage service, S3 to store it's data including training data and other files. You will need to setup and S3 bucket, or S3 compatible service to complete this setup. After, please provide the following details. If you do not have access to S3 bucket, or S3 compatible service, please contact us and we can help setup a temporarly one.  - <code>AWS_ACCESS_URL</code>: AWS S3 endpoint URL - <code>AWS_ACCESS_KEY_ID</code>: AWS access key ID - <code>AWS_SECRET_ACCESS_KEY</code>: AWS secret access key - <code>AWS_S3_BUCKET_DATA</code>: Name of the S3 bucket for data</p>"},{"location":"mlconnector/Installation/#3-postgresql-database","title":"3. PostgreSQL Database","text":"<p>This is used for internal communication of the varrious services. You can setup an external database service if you like. For simplicity you can you use the default values; - <code>POSTGRES_DB</code>: PostgreSQL database name (default, <code>mlmodel</code>) - <code>POSTGRES_USER</code>: PostgreSQL username (default, <code>postgres</code>) - <code>POSTGRES_PASSWORD</code>: PostgreSQL password (default, <code>strongpassword</code>) - <code>PGADMIN_DEFAULT_EMAIL</code>: pgAdmin default login email (default, <code>user@mail.com</code>) - <code>PGADMIN_DEFAULT_PASSWORD</code>: pgAdmin default login password (default, <code>strongpassword</code>) - <code>DB_HOST_NAME</code>: Database host (e.g., <code>database</code>, This corresponds to the name of the container) - <code>DB_PORT</code>: Database port (default: <code>5432</code>) - <code>DB_DRIVER</code>: Database driver string (default, <code>postgresql+asyncpg</code>)  NOTE: Only use an async driver</p>"},{"location":"mlconnector/Installation/#4-northbound-api-endpoint","title":"4. Northbound API Endpoint","text":"<p>The MLConnector  communicates with part of the MYLSyops via the <code>NORTHBOUND_API</code>. Please set this value to the right endpoint. - <code>NORTHBOUND_API_ENDPOINT</code>: Base URL for the Northbound API (e.g., <code>http://your-host:8000</code>)</p>"},{"location":"mlconnector/Installation/#running-the-application","title":"Running the Application","text":"<ol> <li>Start the Docker Containers</li> </ol> <pre><code>docker compose up -d\n</code></pre> <p>This command builds and launches all required services in detached mode.</p> <ol> <li>View Container Logs</li> </ol> <pre><code>docker compose logs -f\n</code></pre>"},{"location":"mlconnector/Installation/#accessing-the-api-documentation","title":"Accessing the API Documentation","text":"<p>Once the services are up and running, open your browser and navigate to:</p> <pre><code>http://&lt;your-host&gt;:8090/redoc\n</code></pre> <p>Replace <code>&lt;your-host&gt;</code> with your server\u2019s hostname or <code>localhost</code> if running locally.</p>"},{"location":"mlconnector/Installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Port Conflicts: Ensure ports <code>8090</code> (API docs) and your database port are available.</li> <li>Environment Variables: Verify all required variables are set. Use <code>docker compose config</code> to inspect the interpolated configuration.</li> <li>Docker Connectivity: Ensure Docker Engine is running and your user has permissions to run Docker commands.</li> <li>API Error Codes: All status codes and error messages can be accessed via: <code>http://&lt;your-host&gt;:8090/redoc</code></li> </ul>"},{"location":"mlconnector/Installation/#license","title":"License","text":""},{"location":"mlconnector/Overview/","title":"MLConnector","text":""},{"location":"mlconnector/Overview/#mlconnector","title":"MLConnector","text":"<p>This section describes the ML API (MLConnector) design. It is based on Flask REST. This is the bridge between all MYSysOps operations and ML assisted operations. It allow for flexible and decoupled way to train, deploy, and monitor all ML operations within the MYSysOps continuum. It also offers surpport for drift detectin and explainability. Below the flow diagram. </p> <p>For installation and step-by-step guide, please checkout the following sections. </p>"},{"location":"mlconnector/Step-by-step%20guide/","title":"MLConnector step-by-step guide and example","text":"<p>Base URL: <code>BASE_URL</code></p>"},{"location":"mlconnector/Step-by-step%20guide/#model-endpoints","title":"Model Endpoints","text":""},{"location":"mlconnector/Step-by-step%20guide/#model-registration","title":"Model Registration","text":"<p>Model registration is a two step process. In the initial step, we add the model metadata using json description defined below. For example, model type, hyperparameter, modeltags and other features. The second step involve adding the model artifacts; .pkl file, training data, requirements file and python script that will be used to retrain the model (See example). </p>"},{"location":"mlconnector/Step-by-step%20guide/#post-modeladd","title":"POST /model/add","text":"<p>Summary: Add new ML model metadata.</p> <p>Request Body (<code>MLModelCreate</code>): <pre><code>{\n  \"modelname\": \"RandomForest\",\n  \"modelkind\": \"classification\",\n  \"drift_detection\": [\n    { \"is_true\": 0, \"method\": 0 }\n  ]\n  // other fields (see endpoint): hyperparameter, modelperformance, trainingresource, runresource, featurelist, inference, modeltags\n}\n</code></pre></p> <p>Responses: - 201: Created <code>MLModel</code> object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X POST \"BASE_URL/model/add\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"modelname\": \"MyModel\",\n           \"modelkind\": \"regression\",\n           \"drift_detection\": [{\"is_true\": 1, \"method\": 2}]\n         }'\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\npayload = {\n    \"modelname\": \"MyModel\",\n    \"modelkind\": \"regression\",\n    \"drift_detection\": [{\"is_true\": 1, \"method\": 2}]\n}\nresp = requests.post(\"BASE_URL/model/add\", json=payload)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: POST /model/add\n    Note right of Agent: Body: MLModelCreate JSON\n    MLConnector--&gt;&gt;Agent: 201 Created</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#post-modelmodel_idupload","title":"POST /model/{model_id}/upload","text":"<p>Summary: Upload a file for a specific model.</p> <p>Path Parameters:</p> Name In Type Required Description model_id path string yes ID of the model <p>Request Body (multipart/form-data): - <code>file</code> (binary) - <code>file_kind</code>: <code>model</code> | <code>data</code> | <code>code</code></p> <p>Responses: - 201: <code>FileSchema</code> object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X POST \"BASE_URL/model/1234/upload\" \\\n     -F \"file=@/path/to/model.pkl\" \\\n     -F \"file_kind=model\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nfiles = {\n    \"file\": open(\"model.pkl\", \"rb\"),\n    \"file_kind\": (None, \"model\")\n}\nresp = requests.post(\"BASE_URL/model/1234/upload\", files=files)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: POST /model/{model_id}/upload\n    Note right of Agent: multipart/form-data (file, file_kind)\n    MLConnector--&gt;&gt;Agent: 201 Created</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#get-modelall","title":"GET /model/all","text":"<p>Summary: Get all ML models.</p> <p>Query Parameters:</p> Name In Type Default Required Description skip query integer 0 no Number of items to skip limit query integer 100 no Maximum number of items <p>Responses: - 200: Array of <code>MLModel</code> objects. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X GET \"BASE_URL/model/all?skip=0&amp;limit=50\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.get(\n    \"BASE_URL/model/all\",\n    params={\"skip\": 0, \"limit\": 50}\n)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /model/all?skip={skip}&amp;limit={limit}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#get-modelgetkindmodelkind","title":"GET /model/getkind/{modelkind}","text":"<p>Summary: Get models by kind.</p> <p>Path Parameters:</p> Name In Type Required Description modelkind path string yes <code>classification</code>, <code>regression</code>, or <code>clustering</code> <p>Responses: - 200: Array of <code>MLModel</code> objects. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X GET \"BASE_URL/model/getkind/regression\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nkind = \"regression\"\nresp = requests.get(f\"BASE_URL/model/getkind/{kind}\")\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /model/getkind/{modelkind}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#get-modelsearch","title":"GET /model/search","text":"<p>Summary: Get models by tags.</p> <p>Query Parameters:</p> Name In Type Required Description tags query array of strings no e.g. <code>?tags=fast&amp;tags=tree-based</code> <p>Responses: - 200: Array of <code>MLModel</code> objects. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -G \"BASE_URL/model/search\" \\\n     --data-urlencode \"tags=fast\" \\\n     --data-urlencode \"tags=accuracy-focused\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nparams = [(\"tags\", \"fast\"), (\"tags\", \"accuracy-focused\")]\nresp = requests.get(\"BASE_URL/model/search\", params=params)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /model/search?tags=tag1&amp;tags=tag2\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#patch-modelmodel_id","title":"PATCH /model/{model_id}","text":"<p>Summary: Update metadata of an existing model.</p> <p>Path Parameters:</p> Name In Type Required Description model_id path string yes ID of the model <p>Note: Request body schema not defined in spec; typically a partial <code>MLModel</code> object.</p> <p>Responses: - 200: (empty response) - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X PATCH \"BASE_URL/model/1234\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"modeltags\": [\"updated-tag\"],\n           \"drift_detection\": [{\"is_true\": 1, \"method\": 1}]\n         }'\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nupdate = {\n    \"modeltags\": [\"updated-tag\"],\n    \"drift_detection\": [{\"is_true\": 1, \"method\": 1}]\n}\nresp = requests.patch(\"BASE_URL/model/1234\", json=update)\nprint(resp.status_code)\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: PATCH /model/{model_id}\n    Note right of Agent: Body: partial MLModel JSON\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#delete-modelmodel_id","title":"DELETE /model/{model_id}","text":"<p>Summary: Delete an existing model.</p> <p>Path Parameters:</p> Name In Type Required Description model_id path string yes ID of the model <p>Responses: - 200: (empty response) - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X DELETE \"BASE_URL/model/1234\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.delete(\"BASE_URL/model/1234\")\nprint(resp.status_code)\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: DELETE /model/{model_id}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#training-endpoints","title":"Training Endpoints","text":""},{"location":"mlconnector/Step-by-step%20guide/#post-mltrainingadd","title":"POST /mltraining/add","text":"<p>Summary: Initiate model training.</p> <p>Request Body (<code>MLTrainCreate</code>): <pre><code>{\n  \"modelid\": \"1234\",\n  \"placement\": {\n    \"clusterID\": \"*\",\n    \"node\": \"*\",\n    \"continuum\": false\n  }\n}\n</code></pre></p> <p>Responses: - 201: <code>MLTrain</code> object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X POST \"BASE_URL/mltraining/add\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"modelid\": \"1234\",\n           \"placement\": { \"clusterID\": \"*\", \"node\": \"*\", \"continuum\": false }\n         }'\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\npayload = {\n    \"modelid\": \"1234\",\n    \"placement\": {\"clusterID\": \"*\", \"node\": \"*\", \"continuum\": False}\n}\nresp = requests.post(\"BASE_URL/mltraining/add\", json=payload)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: POST /mltraining/add\n    Note right of Agent: Body: MLTrainCreate JSON\n    MLConnector--&gt;&gt;Agent: 201 Created</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#deployment-endpoints","title":"Deployment Endpoints","text":""},{"location":"mlconnector/Step-by-step%20guide/#get-deploymentall","title":"GET /deployment/all","text":"<p>Summary: Get all deployments.</p> <p>Query Parameters:</p> Name In Type Default Required Description skip query integer 0 no Number of items to skip limit query integer 100 no Maximum number of items <p>Responses: - 200: Array of deployment objects. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X GET \"BASE_URL/deployment/all?skip=0&amp;limit=50\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.get(\n    \"BASE_URL/deployment/all\",\n    params={\"skip\": 0, \"limit\": 50}\n)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /deployment/all?skip={skip}&amp;limit={limit}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#post-deploymentadd","title":"POST /deployment/add","text":"<p>Summary: Create a new deployment.</p> <p>Request Body (<code>MLDeploymentCreate</code>): <pre><code>{\n  \"modelid\": \"1234\",\n  \"ownerid\": \"agent-1\",\n  \"placement\": { \"clusterID\": \"*\", \"node\": \"*\", \"continuum\": true },\n  \"deployment_id\": \"dep-5678\",\n  \"inference_data\": 1\n}\n</code></pre></p> <p>Responses: - 201: <code>MLDeploymentReturn</code> object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X POST \"BASE_URL/deployment/add\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"modelid\": \"1234\",\n           \"ownerid\": \"agent-1\",\n           \"placement\": { \"clusterID\": \"*\", \"node\": \"*\", \"continuum\": true },\n           \"deployment_id\": \"dep-5678\",\n           \"inference_data\": 1\n         }'\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\npayload = {\n    \"modelid\": \"1234\",\n    \"ownerid\": \"agent-1\",\n    \"placement\": {\"clusterID\": \"*\", \"node\": \"*\", \"continuum\": True},\n    \"deployment_id\": \"dep-5678\",\n    \"inference_data\": 1\n}\nresp = requests.post(\"BASE_URL/deployment/add\", json=payload)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: POST /deployment/add\n    Note right of Agent: Body: MLDeploymentCreate JSON\n    MLConnector--&gt;&gt;Agent: 201 Created</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#post-deploymentaddoperation","title":"POST /deployment/add/operation","text":"<p>Summary: Record an inference operation.</p> <p>Request Body (<code>MLDeploymentOposCreate</code>): <pre><code>{\n  \"ownerid\": \"agent-1\",\n  \"deploymentid\": \"dep-5678\",\n  \"modelid\": \"1234\",\n  \"data\": \"{...}\",\n  \"result\": \"{...}\"\n}\n</code></pre></p> <p>Responses: - 201: <code>MLDeploymentOposReturn</code> object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X POST \"BASE_URL/deployment/add/operation\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"ownerid\": \"agent-1\",\n           \"deploymentid\": \"dep-5678\",\n           \"modelid\": \"1234\",\n           \"data\": \"{...}\",\n           \"result\": \"{...}\"\n         }'\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\npayload = {\n    \"ownerid\": \"agent-1\",\n    \"deploymentid\": \"dep-5678\",\n    \"modelid\": \"1234\",\n    \"data\": \"{...}\",\n    \"result\": \"{...}\"\n}\nresp = requests.post(\"BASE_URL/deployment/add/operation\", json=payload)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: POST /deployment/add/operation\n    Note right of Agent: Body: MLDeploymentOposCreate JSON\n    MLConnector--&gt;&gt;Agent: 201 Created</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#get-deploymentgetstatusdeployment_id","title":"GET /deployment/get/status/{deployment_id}","text":"<p>Summary: Retrieve deployment status.</p> <p>Path Parameters:</p> Name In Type Required Description deployment_id path string yes ID of the deployment <p>Responses: - 200: Status object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X GET \"BASE_URL/deployment/get/status/dep-5678\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.get(\"BASE_URL/deployment/get/status/dep-5678\")\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /deployment/get/status/{deployment_id}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#get-deploymentgetoposownerid","title":"GET /deployment/get/opos/{ownerid}","text":"<p>Summary: List operations by owner.</p> <p>Path Parameters:</p> Name In Type Required Description ownerid path string yes ID of the operation's owner <p>Responses: - 200: Array of <code>MLDeploymentOposReturn</code> objects. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X GET \"BASE_URL/deployment/get/opos/agent-1\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.get(\"BASE_URL/deployment/get/opos/agent-1\")\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /deployment/get/opos/{ownerid}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#delete-deploymentdeployment_id","title":"DELETE /deployment/{deployment_id}","text":"<p>Summary: Delete a deployment.</p> <p>Path Parameters:</p> Name In Type Required Description deployment_id path string yes ID of the deployment <p>Responses: - 200: (empty response) - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X DELETE \"BASE_URL/deployment/dep-5678\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.delete(\"BASE_URL/deployment/dep-5678\")\nprint(resp.status_code)\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: DELETE /deployment/{deployment_id}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#end-to-end-example","title":"End-to-end example","text":"<p>Base URL: <code>BASE_URL</code></p>"},{"location":"mlconnector/Step-by-step%20guide/#1-build-and-save-model","title":"1. Build and save model","text":"<p>Below, we build a simple regression model using scikit-learn and save it to local storage.</p> <pre><code>...\n# Replace with your training pipleline\nreg = Ridge(alpha=1.0, random_state=0)\nreg.fit(X, y)\n...\n\n# It is important that all models are saved with a .pkl extension\n# Serialize  with pickle to a .pkl file\noutput_path = \"diabetes_ridge.pkl\"\nwith open(output_path, \"wb\") as f:\n    pickle.dump(reg, f)\n</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#2-register-ml-model-with","title":"2. Register ML model with","text":""},{"location":"mlconnector/Step-by-step%20guide/#21-model-metadata","title":"2.1 Model metadata","text":"<p>To register the model above, first we add the model metadata and then the model artfacts. Using the model above, here is json description example (To see what each parameter means see api documentation). <pre><code>{\n  \"modelname\": \"Ridge\",\n  \"modelkind\": \"Regressor\",\n  \"hyperparameter\": [\n    {\n      \"parameter\": \"string\",\n      \"value\": 0\n    }\n  ],\n  \"modelperformance\": [\n    {\n      \"metric\": \"Accuracy\",\n      \"order\": 1,\n      \"threshold\": 0.89\n    }\n  ],\n  \"trainingresource\": [\n    {\n      \"resource_name\": \"GPU\",\n      \"value\": 16,\n      \"deploy\": \"string\"\n    }\n  ],\n  \"runresource\": [\n    {\n      \"resource_name\": \"GPU\",\n      \"value\": 16,\n      \"deploy\": \"string\"\n    }\n  ],\n  \"featurelist\": [...],\n  \"inference\": [\n    {\n      \"type\": \"string\",\n      \"value\": \"string\"\n    }\n  ],\n\"modeltags\": [\n    \"regression\",\n       \"fast\"\n  ],\n\"drift_detection\": [\n    {\n      \"is_true\": 1,\n      \"method\": 0\n    }\n  ]\n}\n</code></pre> Use the above description, we can then make a post request to register the model. </p> <pre><code>import requests\nresp = requests.post(\"BASE_URL/model/add\", json=payload)\nprint(resp.json())\n</code></pre>"},{"location":"mlconnector/Step-by-step%20guide/#22-model-artifacts","title":"2.2 Model artifacts","text":"<p>The above step should return a model_id that will be used in the next steps. Here, will upload the model artifacts. These include;  - Model file (pickled file saved in step one above) - Training data. This will be used for explainability and drift detection. (Note, it has to be the exact same data used to train the model, otherwise you will get wrong results) - Requirements file that defines the environment the model was trained in.</p> <p>Upload these one by one using the example bellow; Note: file_kind can be <code>model</code>, <code>data</code>, <code>code</code>, and <code>env</code> <pre><code>import requests\n\nfiles = {\n    \"file\": open(\"model.pkl\", \"rb\"),\n    \"file_kind\": (None, \"model\")\n}\nresp = requests.post(\"BASE_URL/model/1234/upload\", files=files)\nprint(resp.json())\n</code></pre></p>"},{"location":"mlconnector/Step-by-step%20guide/#3-deployment","title":"3. Deployment","text":"<p>After adding the model artifacts, the next step is to deploy the model. The ML model is deployed as standalone docker application and an endpoint is returned to which inference data can be passed.  <pre><code>import requests\n\npayload = {\n    \"modelid\": \"1234\",\n    \"ownerid\": \"agent-1\",\n    \"placement\": {..},\n    \"deployment_id\": \"\",\n    \"inference_data\": 1\n}\nresp = requests.post(\"BASE_URL/deployment/add\", json=payload)\nprint(resp.json())\n</code></pre> <code>placement</code> can one of the following;  - Placement to a specific cluster, node and continuum <pre><code>{\"clusterID\": \"UTH-Internal-testbed\", \"node\": \"mls-drone\", \"continuum\": \"Edge\"}\n</code></pre> - Placement on a given cluster <pre><code>  {\"clusterID\": \"UTH-Internal-testbed\", \"node\": \"*\", \"continuum\": \"*\"}\n</code></pre> - Placement anywhere <pre><code>{\"clusterID\": \"*\", \"node\": \"*\", \"continuum\": \"*\"}\n</code></pre> This returns a deployment_id used to query the status of the deployment and also the inference endpoint and explainability. </p>"},{"location":"mlconnector/Step-by-step%20guide/#31-query-deployment-status","title":"3.1 Query Deployment Status","text":"<ul> <li>List All: <code>GET /deployment/all?skip={skip}&amp;limit={limit}</code> </li> <li>Get Status: <code>GET /deployment/get/status/{deployment_id}</code></li> </ul> <p>Example: <pre><code>curl -X GET \"BASE_URL/deployment/get/status/dep-iris-001\"\n</code></pre></p>"},{"location":"mlconnector/Step-by-step%20guide/#4-inference-endpoint-including-explainability","title":"4. Inference Endpoint (including Explainability)","text":""},{"location":"mlconnector/Step-by-step%20guide/#41-predict-call","title":"4.1 Predict Call","text":"<p>Assuming deployment created with <code>deployment_id = dep-iris-001</code>:</p> <pre><code>curl -X POST \"BASE_URL/deployment/dep-iris-001/predict\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"data\": (5.1, 3.5, 1.4, 0.2),\n           \"explain\": true\n         }'\n</code></pre> <p>Response: <pre><code>{\n  \"prediction\": [0],\n  \"explanation\": {\n    \"feature_importance\": [0.12, 0.08, 0.70, 0.10],\n    \"method\": \"shap\"\n  }\n}\n</code></pre></p>"},{"location":"mlconnector/Step-by-step%20guide/#42-explainability-details","title":"4.2 Explainability Details","text":"<ul> <li>When <code>explain=true</code>, response includes per-feature contributions (e.g., SHAP values).  </li> <li>Interpretation: Positive values push toward the predicted class; negatives push away.  </li> </ul>"},{"location":"references/cli/","title":"Command-line Interfaces","text":"<p>The MLS CLI is a Python-based command-line tool designed for managing application deployments and system interactions within the MLSysOps framework. It provides functionalities for deploying and managing applications, infrastructure components, and machine learning (ML) models, as well as for querying system status and monitoring deployments. The CLI communicates with the Northbound API, enabling efficient interaction with the MLSysOps framework. Ke y functionalities provided by the MLS CLI through the NB API include service health checks (ping), application deployment, and retrieval of system, application, and ML model statuses. This tool streamlines deployment and management workflows, offering an intuitive interface for interacting with the agent-based framework while ensuring efficient system operations.</p> <p>The CLI is organized into distinct command groups, each responsible for managing a specific aspect of the system: - apps: Manage application deployment, monitoring, and removal - infra: Register and manage infrastructure components across the continuum - ml: Handle the deployment and lifecycle of machine learning models - manage: Perform general system operations, such as health checks and mode switching</p> <p>This structured CLI design ensures that different user roles can efficiently interact with the system based on their specific needs, further reinforcing the modular and scalable nature of the MLSysOps framework.</p> <p>The table presents an overview of the CLI commands currently available. These commands are indicative and may be updated or extended in the open-source release.</p> Group Command Description Parameters APP deploy-app Deploy an application using a YAML file YAML file using path or URI list-all List the applications on the system - get-app-status Get the status of the application App_id get-app-details Get the details of an application App_id get-app-performance Get the performance metric of an application App_id remove-app Remove an application from the system App_id INFRA register-infra Register system description YAML file using path or URI list List infrastructure registered infra_id (Datacenter or cluster ID) unregister-infra Remove system description infra_id Management /config set-mode Change between ML or Heuristic-normal mode 0 for Heuristic, 1 for ML Set System Target Set infrastructure level targets List of IDs and list of targets Config Trust Configure trust assessment List of node IDs, list of indexes, and list of weights Ping Ping the continuum agent - ML deploy-ml Deploy an ML application using a YAML file YAML file using path or URI list-all List the ML models deployed on the system - get-status Get the status of the ML models model_uid remove-ml Remove an ML model from the system model_uid"},{"location":"references/ml-connector/","title":"API Integration Documentation","text":"<p>Base URL: <code>BASE_URL</code></p>"},{"location":"references/ml-connector/#model-endpoints","title":"Model Endpoints","text":""},{"location":"references/ml-connector/#model-registration","title":"Model Registration","text":"<p>Model registration is a two step process. In the initial step, we add the model metadata using json description defined below. For example, model type, hyperparameter, modeltags and other features. The second step involve adding the model artifacts; .pkl file, training data, requirements file and python script that will be used to retrain the model (See example). </p>"},{"location":"references/ml-connector/#post-modeladd","title":"POST /model/add","text":"<p>Summary: Add new ML model metadata.</p> <p>Request Body (<code>MLModelCreate</code>): <pre><code>{\n  \"modelname\": \"RandomForest\",\n  \"modelkind\": \"classification\",\n  \"drift_detection\": [\n    { \"is_true\": 0, \"method\": 0 }\n  ]\n  // other fields (see endpoint): hyperparameter, modelperformance, trainingresource, runresource, featurelist, inference, modeltags\n}\n</code></pre></p> <p>Responses: - 201: Created <code>MLModel</code> object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X POST \"BASE_URL/model/add\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"modelname\": \"MyModel\",\n           \"modelkind\": \"regression\",\n           \"drift_detection\": [{\"is_true\": 1, \"method\": 2}]\n         }'\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\npayload = {\n    \"modelname\": \"MyModel\",\n    \"modelkind\": \"regression\",\n    \"drift_detection\": [{\"is_true\": 1, \"method\": 2}]\n}\nresp = requests.post(\"BASE_URL/model/add\", json=payload)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: POST /model/add\n    Note right of Agent: Body: MLModelCreate JSON\n    MLConnector--&gt;&gt;Agent: 201 Created</code></pre>"},{"location":"references/ml-connector/#post-modelmodel_idupload","title":"POST /model/{model_id}/upload","text":"<p>Summary: Upload a file for a specific model.</p> <p>Path Parameters:</p> Name In Type Required Description model_id path string yes ID of the model <p>Request Body (multipart/form-data): - <code>file</code> (binary) - <code>file_kind</code>: <code>model</code> | <code>data</code> | <code>code</code></p> <p>Responses: - 201: <code>FileSchema</code> object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X POST \"BASE_URL/model/1234/upload\" \\\n     -F \"file=@/path/to/model.pkl\" \\\n     -F \"file_kind=model\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nfiles = {\n    \"file\": open(\"model.pkl\", \"rb\"),\n    \"file_kind\": (None, \"model\")\n}\nresp = requests.post(\"BASE_URL/model/1234/upload\", files=files)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: POST /model/{model_id}/upload\n    Note right of Agent: multipart/form-data (file, file_kind)\n    MLConnector--&gt;&gt;Agent: 201 Created</code></pre>"},{"location":"references/ml-connector/#get-modelall","title":"GET /model/all","text":"<p>Summary: Get all ML models.</p> <p>Query Parameters:</p> Name In Type Default Required Description skip query integer 0 no Number of items to skip limit query integer 100 no Maximum number of items <p>Responses: - 200: Array of <code>MLModel</code> objects. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X GET \"BASE_URL/model/all?skip=0&amp;limit=50\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.get(\n    \"BASE_URL/model/all\",\n    params={\"skip\": 0, \"limit\": 50}\n)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /model/all?skip={skip}&amp;limit={limit}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"references/ml-connector/#get-modelgetkindmodelkind","title":"GET /model/getkind/{modelkind}","text":"<p>Summary: Get models by kind.</p> <p>Path Parameters:</p> Name In Type Required Description modelkind path string yes <code>classification</code>, <code>regression</code>, or <code>clustering</code> <p>Responses: - 200: Array of <code>MLModel</code> objects. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X GET \"BASE_URL/model/getkind/regression\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nkind = \"regression\"\nresp = requests.get(f\"BASE_URL/model/getkind/{kind}\")\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /model/getkind/{modelkind}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"references/ml-connector/#get-modelsearch","title":"GET /model/search","text":"<p>Summary: Get models by tags.</p> <p>Query Parameters:</p> Name In Type Required Description tags query array of strings no e.g. <code>?tags=fast&amp;tags=tree-based</code> <p>Responses: - 200: Array of <code>MLModel</code> objects. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -G \"BASE_URL/model/search\" \\\n     --data-urlencode \"tags=fast\" \\\n     --data-urlencode \"tags=accuracy-focused\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nparams = [(\"tags\", \"fast\"), (\"tags\", \"accuracy-focused\")]\nresp = requests.get(\"BASE_URL/model/search\", params=params)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /model/search?tags=tag1&amp;tags=tag2\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"references/ml-connector/#patch-modelmodel_id","title":"PATCH /model/{model_id}","text":"<p>Summary: Update metadata of an existing model.</p> <p>Path Parameters:</p> Name In Type Required Description model_id path string yes ID of the model <p>Note: Request body schema not defined in spec; typically a partial <code>MLModel</code> object.</p> <p>Responses: - 200: (empty response) - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X PATCH \"BASE_URL/model/1234\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"modeltags\": [\"updated-tag\"],\n           \"drift_detection\": [{\"is_true\": 1, \"method\": 1}]\n         }'\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nupdate = {\n    \"modeltags\": [\"updated-tag\"],\n    \"drift_detection\": [{\"is_true\": 1, \"method\": 1}]\n}\nresp = requests.patch(\"BASE_URL/model/1234\", json=update)\nprint(resp.status_code)\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: PATCH /model/{model_id}\n    Note right of Agent: Body: partial MLModel JSON\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"references/ml-connector/#delete-modelmodel_id","title":"DELETE /model/{model_id}","text":"<p>Summary: Delete an existing model.</p> <p>Path Parameters:</p> Name In Type Required Description model_id path string yes ID of the model <p>Responses: - 200: (empty response) - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X DELETE \"BASE_URL/model/1234\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.delete(\"BASE_URL/model/1234\")\nprint(resp.status_code)\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: DELETE /model/{model_id}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"references/ml-connector/#training-endpoints","title":"Training Endpoints","text":""},{"location":"references/ml-connector/#post-mltrainingadd","title":"POST /mltraining/add","text":"<p>Summary: Initiate model training.</p> <p>Request Body (<code>MLTrainCreate</code>): <pre><code>{\n  \"modelid\": \"1234\",\n  \"placement\": {\n    \"clusterID\": \"*\",\n    \"node\": \"*\",\n    \"continuum\": false\n  }\n}\n</code></pre></p> <p>Responses: - 201: <code>MLTrain</code> object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X POST \"BASE_URL/mltraining/add\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"modelid\": \"1234\",\n           \"placement\": { \"clusterID\": \"*\", \"node\": \"*\", \"continuum\": false }\n         }'\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\npayload = {\n    \"modelid\": \"1234\",\n    \"placement\": {\"clusterID\": \"*\", \"node\": \"*\", \"continuum\": False}\n}\nresp = requests.post(\"BASE_URL/mltraining/add\", json=payload)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: POST /mltraining/add\n    Note right of Agent: Body: MLTrainCreate JSON\n    MLConnector--&gt;&gt;Agent: 201 Created</code></pre>"},{"location":"references/ml-connector/#deployment-endpoints","title":"Deployment Endpoints","text":""},{"location":"references/ml-connector/#get-deploymentall","title":"GET /deployment/all","text":"<p>Summary: Get all deployments.</p> <p>Query Parameters:</p> Name In Type Default Required Description skip query integer 0 no Number of items to skip limit query integer 100 no Maximum number of items <p>Responses: - 200: Array of deployment objects. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X GET \"BASE_URL/deployment/all?skip=0&amp;limit=50\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.get(\n    \"BASE_URL/deployment/all\",\n    params={\"skip\": 0, \"limit\": 50}\n)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /deployment/all?skip={skip}&amp;limit={limit}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"references/ml-connector/#post-deploymentadd","title":"POST /deployment/add","text":"<p>Summary: Create a new deployment.</p> <p>Request Body (<code>MLDeploymentCreate</code>): <pre><code>{\n  \"modelid\": \"1234\",\n  \"ownerid\": \"agent-1\",\n  \"placement\": { \"clusterID\": \"*\", \"node\": \"*\", \"continuum\": true },\n  \"deployment_id\": \"dep-5678\",\n  \"inference_data\": 1\n}\n</code></pre></p> <p>Responses: - 201: <code>MLDeploymentReturn</code> object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X POST \"BASE_URL/deployment/add\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"modelid\": \"1234\",\n           \"ownerid\": \"agent-1\",\n           \"placement\": { \"clusterID\": \"*\", \"node\": \"*\", \"continuum\": true },\n           \"deployment_id\": \"dep-5678\",\n           \"inference_data\": 1\n         }'\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\npayload = {\n    \"modelid\": \"1234\",\n    \"ownerid\": \"agent-1\",\n    \"placement\": {\"clusterID\": \"*\", \"node\": \"*\", \"continuum\": True},\n    \"deployment_id\": \"dep-5678\",\n    \"inference_data\": 1\n}\nresp = requests.post(\"BASE_URL/deployment/add\", json=payload)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: POST /deployment/add\n    Note right of Agent: Body: MLDeploymentCreate JSON\n    MLConnector--&gt;&gt;Agent: 201 Created</code></pre>"},{"location":"references/ml-connector/#post-deploymentaddoperation","title":"POST /deployment/add/operation","text":"<p>Summary: Record an inference operation.</p> <p>Request Body (<code>MLDeploymentOposCreate</code>): <pre><code>{\n  \"ownerid\": \"agent-1\",\n  \"deploymentid\": \"dep-5678\",\n  \"modelid\": \"1234\",\n  \"data\": \"{...}\",\n  \"result\": \"{...}\"\n}\n</code></pre></p> <p>Responses: - 201: <code>MLDeploymentOposReturn</code> object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X POST \"BASE_URL/deployment/add/operation\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"ownerid\": \"agent-1\",\n           \"deploymentid\": \"dep-5678\",\n           \"modelid\": \"1234\",\n           \"data\": \"{...}\",\n           \"result\": \"{...}\"\n         }'\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\npayload = {\n    \"ownerid\": \"agent-1\",\n    \"deploymentid\": \"dep-5678\",\n    \"modelid\": \"1234\",\n    \"data\": \"{...}\",\n    \"result\": \"{...}\"\n}\nresp = requests.post(\"BASE_URL/deployment/add/operation\", json=payload)\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: POST /deployment/add/operation\n    Note right of Agent: Body: MLDeploymentOposCreate JSON\n    MLConnector--&gt;&gt;Agent: 201 Created</code></pre>"},{"location":"references/ml-connector/#get-deploymentgetstatusdeployment_id","title":"GET /deployment/get/status/{deployment_id}","text":"<p>Summary: Retrieve deployment status.</p> <p>Path Parameters:</p> Name In Type Required Description deployment_id path string yes ID of the deployment <p>Responses: - 200: Status object. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X GET \"BASE_URL/deployment/get/status/dep-5678\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.get(\"BASE_URL/deployment/get/status/dep-5678\")\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /deployment/get/status/{deployment_id}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"references/ml-connector/#get-deploymentgetoposownerid","title":"GET /deployment/get/opos/{ownerid}","text":"<p>Summary: List operations by owner.</p> <p>Path Parameters:</p> Name In Type Required Description ownerid path string yes ID of the operation's owner <p>Responses: - 200: Array of <code>MLDeploymentOposReturn</code> objects. - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X GET \"BASE_URL/deployment/get/opos/agent-1\" \\\n     -H \"Accept: application/json\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.get(\"BASE_URL/deployment/get/opos/agent-1\")\nprint(resp.json())\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: GET /deployment/get/opos/{ownerid}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"references/ml-connector/#delete-deploymentdeployment_id","title":"DELETE /deployment/{deployment_id}","text":"<p>Summary: Delete a deployment.</p> <p>Path Parameters:</p> Name In Type Required Description deployment_id path string yes ID of the deployment <p>Responses: - 200: (empty response) - 422: <code>HTTPValidationError</code>.</p> <p>Example cURL: <pre><code>curl -X DELETE \"BASE_URL/deployment/dep-5678\"\n</code></pre></p> <p>Example Python: <pre><code>import requests\n\nresp = requests.delete(\"BASE_URL/deployment/dep-5678\")\nprint(resp.status_code)\n</code></pre></p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MLConnector\n    Agent-&gt;&gt;MLConnector: DELETE /deployment/{deployment_id}\n    MLConnector--&gt;&gt;Agent: 200 OK</code></pre>"},{"location":"references/ml-connector/#end-to-end-example","title":"End-to-end example","text":"<p>Base URL: <code>BASE_URL</code></p>"},{"location":"references/ml-connector/#1-build-and-save-model","title":"1. Build and save model","text":"<p>Below, we build a simple regression model using scikit-learn and save it to local storage.</p> <pre><code>...\n# Replace with your training pipleline\nreg = Ridge(alpha=1.0, random_state=0)\nreg.fit(X, y)\n...\n\n# It is important that all models are saved with a .pkl extension\n# Serialize  with pickle to a .pkl file\noutput_path = \"diabetes_ridge.pkl\"\nwith open(output_path, \"wb\") as f:\n    pickle.dump(reg, f)\n</code></pre>"},{"location":"references/ml-connector/#2-register-ml-model-with","title":"2. Register ML model with","text":""},{"location":"references/ml-connector/#21-model-metadata","title":"2.1 Model metadata","text":"<p>To register the model above, first we add the model metadata and then the model artfacts. Using the model above, here is json description example (To see what each parameter means see api documentation). <pre><code>{\n  \"modelname\": \"Ridge\",\n  \"modelkind\": \"Regressor\",\n  \"hyperparameter\": [\n    {\n      \"parameter\": \"string\",\n      \"value\": 0\n    }\n  ],\n  \"modelperformance\": [\n    {\n      \"metric\": \"Accuracy\",\n      \"order\": 1,\n      \"threshold\": 0.89\n    }\n  ],\n  \"trainingresource\": [\n    {\n      \"resource_name\": \"GPU\",\n      \"value\": 16,\n      \"deploy\": \"string\"\n    }\n  ],\n  \"runresource\": [\n    {\n      \"resource_name\": \"GPU\",\n      \"value\": 16,\n      \"deploy\": \"string\"\n    }\n  ],\n  \"featurelist\": [...],\n  \"inference\": [\n    {\n      \"type\": \"string\",\n      \"value\": \"string\"\n    }\n  ],\n\"modeltags\": [\n    \"regression\",\n       \"fast\"\n  ],\n\"drift_detection\": [\n    {\n      \"is_true\": 1,\n      \"method\": 0\n    }\n  ]\n}\n</code></pre> Use the above description, we can then make a post request to register the model. </p> <pre><code>import requests\nresp = requests.post(\"BASE_URL/model/add\", json=payload)\nprint(resp.json())\n</code></pre>"},{"location":"references/ml-connector/#22-model-artifacts","title":"2.2 Model artifacts","text":"<p>The above step should return a model_id that will be used in the next steps. Here, will upload the model artifacts. These include;  - Model file (pickled file saved in step one above) - Training data. This will be used for explainability and drift detection. (Note, it has to be the exact same data used to train the model, otherwise you will get wrong results) - Requirements file that defines the environment the model was trained in.</p> <p>Upload these one by one using the example bellow; Note: file_kind can be <code>model</code>, <code>data</code>, <code>code</code>, and <code>env</code> <pre><code>import requests\n\nfiles = {\n    \"file\": open(\"model.pkl\", \"rb\"),\n    \"file_kind\": (None, \"model\")\n}\nresp = requests.post(\"BASE_URL/model/1234/upload\", files=files)\nprint(resp.json())\n</code></pre></p>"},{"location":"references/ml-connector/#3-deployment","title":"3. Deployment","text":"<p>After adding the model artifacts, the next step is to deploy the model. The ML model is deployed as standalone docker application and an endpoint is returned to which inference data can be passed.  <pre><code>import requests\n\npayload = {\n    \"modelid\": \"1234\",\n    \"ownerid\": \"agent-1\",\n    \"placement\": {..},\n    \"deployment_id\": \"\",\n    \"inference_data\": 1\n}\nresp = requests.post(\"BASE_URL/deployment/add\", json=payload)\nprint(resp.json())\n</code></pre> <code>placement</code> can one of the following;  - Placement to a specific cluster, node and continuum <pre><code>{\"clusterID\": \"UTH-Internal-testbed\", \"node\": \"mls-drone\", \"continuum\": \"Edge\"}\n</code></pre> - Placement on a given cluster <pre><code>  {\"clusterID\": \"UTH-Internal-testbed\", \"node\": \"*\", \"continuum\": \"*\"}\n</code></pre> - Placement anywhere <pre><code>{\"clusterID\": \"*\", \"node\": \"*\", \"continuum\": \"*\"}\n</code></pre> This returns a deployment_id used to query the status of the deployment and also the inference endpoint and explainability. </p>"},{"location":"references/ml-connector/#31-query-deployment-status","title":"3.1 Query Deployment Status","text":"<ul> <li>List All: <code>GET /deployment/all?skip={skip}&amp;limit={limit}</code> </li> <li>Get Status: <code>GET /deployment/get/status/{deployment_id}</code></li> </ul> <p>Example: <pre><code>curl -X GET \"BASE_URL/deployment/get/status/dep-iris-001\"\n</code></pre></p>"},{"location":"references/ml-connector/#4-inference-endpoint-including-explainability","title":"4. Inference Endpoint (including Explainability)","text":""},{"location":"references/ml-connector/#41-predict-call","title":"4.1 Predict Call","text":"<p>Assuming deployment created with <code>deployment_id = dep-iris-001</code>:</p> <pre><code>curl -X POST \"BASE_URL/deployment/dep-iris-001/predict\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"data\": (5.1, 3.5, 1.4, 0.2),\n           \"explain\": true\n         }'\n</code></pre> <p>Response: <pre><code>{\n  \"prediction\": [0],\n  \"explanation\": {\n    \"feature_importance\": [0.12, 0.08, 0.70, 0.10],\n    \"method\": \"shap\"\n  }\n}\n</code></pre></p>"},{"location":"references/ml-connector/#42-explainability-details","title":"4.2 Explainability Details","text":"<ul> <li>When <code>explain=true</code>, response includes per-feature contributions (e.g., SHAP values).  </li> <li>Interpretation: Positive values push toward the predicted class; negatives push away.  </li> </ul>"},{"location":"references/northbound-api/","title":"Northbound API Reference","text":"<p>The Northbound API (NB API) serves as the main interface for external systems\u2014such as user interfaces and automation tools\u2014to interact with the MLSysOps agent-based orchestration framework. Designed as an HTTP-based RESTful service, it enables users to send commands, retrieve information, and monitor overall system status. The NB API operates on a predefined IP and port, supporting secure, asynchronous communication with the Continuum Agent. This design allows for a modular and scalable control layer, effectively abstracting the internal complexity of the multi-agent system. As a result, it offers a clean, service-oriented interface for seamless integration with external management tools.</p> <p>To ensure clarity and maintainability, the NB API is structured into four main categories, each aligned with a specific operational domain of the system. This modular organization reflects the core responsibilities and lifecycle stages of the MLSysOps framework, facilitating consistent and intuitive interaction for all users and systems.</p> <p>Applications: Manage the lifecycle of deployed applications\u2014from deployment to monitoring and removal.</p> Method Endpoint Description POST /apps/deploy Deploy an application. Requires app description in request body. GET /apps/list_all/ Retrieve a list of all deployed applications in the framework. GET /apps/status/{app_id} Get the current status of a specific application. GET /apps/apps/details/{app_id} Fetch detailed metadata of an application. GET /apps/performance/{app_id} Access performance metrics of a deployed application. DELETE /apps/remove/{app_id} Remove (undeploy) a specific application. <p>ML Models: Control the deployment and lifecycle of machine learning models integrated into the system.</p> Method Endpoint Description POST /ml/deploy_ml Deploy a machine learning model to the infrastructure. GET /ml/list_all/ List all currently deployed ML models. GET /ml/status/{model_uid} Check the status of deployment of an ML model. DELETE /ml/remove/{model_uid} Remove an ML model from the system. <p>Infrastructure: Register, list, and manage edge, cluster, and datacenter components that make up the continuum.</p> Method Endpoint Description POST /infra/register Register infrastructure components (edge node, cluster, etc.). GET /infra/list/ List all registered infrastructure components. DELETE /infra/unregister/{infra_id} Unregister and remove an infrastructure component. <p>Management: System-level controls for health checks and operational mode switching.</p> Method Endpoint Description GET /manage/ping Check continuum agent status (ping the continuum agent). PUT /manage/mode/{mode} Change operational mode of the Agent (Heuristic or ML)."},{"location":"references/telemetrysdk/","title":"MLSysOps Telemetry SDK &amp; API","text":"<p>The MLSysOps framework offers integrators two pathways to interface with the telemetry system: i) use the OpenTelemetry SDK and its respective API or ii) employ the MLSysOps Telemetry SDK, which provides a simplified API. The former provides SDKs for a wide range of programming languages, enabling both manual instrumentation and automated instrumentation on compatible software systems. The latter serves as a wrapper on top of the OpenTelemetry SDK, abstracting away all the mundane code required to connect to an OpenTelemetry Collector and push metrics. This abstraction uses two function calls: one for pushing metrics and one for retrieving metrics. Instrumenting application components within the MLSysOps Framework is achievable with either of these options. MLSysOps Telemetry API/SDK is implemented in Python language and is available in the opensource repository MLSysOps Python Telemetry Library.  </p>"},{"location":"references/telemetrysdk/#api-reference","title":"API Reference","text":"<p>TBD</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>In this section we include some end-to-end tutorials on deploying <code>MLSysOps</code> applications to a freshly installed environment.</p>"},{"location":"user-guide/app-system-descriptions/","title":"Application and System Description","text":"<p>Note: For more information about the type for each property refer to the corresponding CRD file.</p>"},{"location":"user-guide/app-system-descriptions/#application-custom-resource-appyaml-reference","title":"Application Custom Resource (<code>app.yaml</code>) Reference","text":""},{"location":"user-guide/app-system-descriptions/#this-section-provides-a-full-example-of-the-appyaml-custom-resource-along-with-a-structured-field-reference-table-derived-from-the-mlsysopsapplicationyaml-crd","title":"This section provides a full example of the <code>app.yaml</code> custom resource along with a structured field reference table derived from the <code>MLSysOpsApplication.yaml</code> CRD.","text":""},{"location":"user-guide/app-system-descriptions/#example-appyaml","title":"Example <code>app.yaml</code>","text":"<pre><code>MLSysOpsApp:\n  name: test-application\n  cluster_placement:\n    cluster_id: [\"uth-prod-cluster\"]\n  components:\n    - metadata:\n        name: server-app\n        uid: a9jwduj9028uje\n      node_placement:\n        continuum_layer:\n          - edge\n        node: node-1 # Replace with proper hostname\n        mobile: True\n      runtime_class_name: nvidia\n      sensors:\n        - camera:\n            model: d455\n            camera_type: rgb\n            minimum_framerate: 20\n            resolution: 1024x768\n      restart_policy: OnFailure\n      node_type: virtualized\n      os: ubuntu\n      container_runtime: containerd\n      containers:\n        - image: harbor.nbfc.io/mlsysops/test-app:latest\n          platform_requirements:\n            cpu: \n              requests: \"250m\"\n              limits: \"500m\"\n              architecture:\n                - amd64\n              frequency: 1.4\n              performance_indicator: 30 # BogoMIPS\n            memory:\n              requests: \"64Mi\"\n              limits: \"128Mi\"\n            disk: \"120\"\n          image_pull_policy: IfNotPresent\n          command: [\"python\", \"TcpServer.py\"]\n          env:\n            - name: OTEL_RESOURCE_ATTRIBUTES\n              value: \"service.name=server-app, service.version=0.0.0, service.experimentid=test\"\n            - name: OTEL_SERVICE_NAME\n              value: \"server-app\"\n            - name: NODE_IP\n              value_from:\n                field_ref:\n                  field_path: status.hostIP\n            - name: TELEMETRY_ENDPOINT # Add code for this to be done dynamically\n              value: \"$(NODE_IP):43170\"\n            - name: TCP_SERVER_IP \n              value: \"0.0.0.0\"\n          ports:\n            - container_port: 10000\n              protocol: TCP\n      qos_metrics:\n        - application_metric_id: test_received_success_counter\n          target: 20\n          relation: lower_or_equal\n          system_metrics_hints:\n            - cpu_frequency\n      host_network: False\n    - metadata:\n        name: client-app\n        uid: jdaddwewed235uje\n      node_placement:\n        mobile: False\n        continuum_layer:\n          - edge\n      sensors:\n        - temperature:\n            model: sdc30\n      restart_policy: OnFailure\n      node_type: native\n      os: ubuntu # Just for demonstration purposes.\n      container_runtime: containerd\n      containers:\n        - image: harbor.nbfc.io/mlsysops/test-app:latest\n          platform_requirements:\n            cpu: #change to cpu and merge with resources above.\n              requests: \"250m\"\n              limits: \"500m\"\n              architecture:\n                - arm64\n              frequency: 1.4\n            memory:\n              requests: \"64Mi\"\n              limits: \"128Mi\"\n            disk: \"100\"\n            gpu:\n              model: k80\n              memory: 2\n              performance_indicator: 320 # BogoMIPS\n          image_pull_policy: IfNotPresent\n          command: [\"python\", \"TcpClient.py\"]\n          env:\n            - name: OTEL_RESOURCE_ATTRIBUTES\n              value: \"service.name=server-app, service.version=0.0.0, service.experimentid=test\"\n            - name: OTEL_SERVICE_NAME\n              value: \"server-app\"\n            - name: NODE_IP\n              value_from:\n                field_ref:\n                  field_path: status.hostIP\n            - name: TELEMETRY_ENDPOINT\n              value: \"$(NODE_IP):43170\"\n            - name: TCP_SERVER_IP\n              value: \"server-app\"\n      qos_metrics:\n        - application_metric_id: test_sent_success_counter\n          target: 30\n          relation: equal  \n  component_interactions:\n    - component_name1: client-app\n      type: egress\n      component_name2: server-app\n  global_satisfaction:\n    threshold: 0.7\n    relation: greater_than\n    achievement_weights:\n      - metric_id: test_received_success_counter\n        weight: 0.5\n      - metric_id: test_sent_success_counter\n        weight: 0.5\n</code></pre>"},{"location":"user-guide/app-system-descriptions/#field-reference-table-hierarchical-view","title":"Field Reference Table (Hierarchical View)","text":""},{"location":"user-guide/app-system-descriptions/#top-level-fields","title":"Top-Level Fields","text":"Field Description Required Allowed Values <code>name</code> The application name. Yes - <code>cluster_placement.cluster_id</code> Array of clusters that can host the application. No - <code>components</code> List of components of the application. Yes - <code>component_interactions</code> Describes how components communicate. No - <code>global_satisfaction</code> Global constraints for application satisfaction. No -"},{"location":"user-guide/app-system-descriptions/#componentsmetadata","title":"<code>components[].metadata</code>","text":"Field Description Required Allowed Values <code>name</code> The unique name of the component. Yes - <code>uid</code> Unique identifier (not user-defined). Yes -"},{"location":"user-guide/app-system-descriptions/#componentsnode_placement","title":"<code>components[].node_placement</code>","text":"Field Description Required Allowed Values <code>continuum_layer</code> Required component placement on the continuum. No <code>cloud</code>, <code>far_edge</code>, <code>edge_infrastructure</code>, <code>edge</code>, <code>*</code> <code>mobile</code> Whether component is deployed on a mobile node. No <code>True</code>, <code>False</code> <code>labels</code> Required labels for filtering. No - <code>node</code> Required node name (optional). No -"},{"location":"user-guide/app-system-descriptions/#componentssensors","title":"<code>components[].sensors[]</code>","text":"Field Description Required Allowed Values <code>camera.model</code> Camera sensor model. No <code>d455</code>, <code>imx477</code>, <code>picamera-v2</code> <code>camera.camera_type</code> Type of camera sensor. No <code>rgb</code>, <code>nir</code>, <code>thermal</code>, <code>monocular</code> <code>camera.minimum_framerate</code> Minimum framerate. No - <code>camera.resolution</code> Camera resolution. No <code>1024x768</code>, <code>4056x3040</code> <code>temperature.model</code> Temperature sensor model. No <code>sdc30</code>, <code>ds18b20</code>"},{"location":"user-guide/app-system-descriptions/#componentscontainers","title":"<code>components[].containers[]</code>","text":"Field Description Required Allowed Values <code>image</code> Name of the container image. Yes - <code>command</code> Container startup command. No - <code>image_pull_policy</code> Image pull policy. No <code>Always</code>, <code>Never</code>, <code>IfNotPresent</code>"},{"location":"user-guide/app-system-descriptions/#componentscontainersplatform_requirements","title":"<code>components[].containers[].platform_requirements</code>","text":"Field Description Required Allowed Values <code>cpu.requests</code> CPU requests. No - <code>cpu.limits</code> CPU limits. No - <code>cpu.architecture</code> Supported architectures. No <code>arm64</code>, <code>amd64</code> <code>cpu.frequency</code> Required CPU frequency in Hz. No - <code>cpu.performance_indicator</code> CPU performance hint. No - <code>memory.requests</code> Memory requests. No - <code>memory.limits</code> Memory limits. No - <code>disk</code> Required disk space in GB. No - <code>gpu.model</code> GPU model. No <code>k80</code>, <code>k40</code> <code>gpu.memory</code> GPU memory in GB. No - <code>gpu.performance_indicator</code> GPU performance hint. No -"},{"location":"user-guide/app-system-descriptions/#componentscontainersports","title":"<code>components[].containers[].ports[]</code>","text":"Field Description Required Allowed Values <code>container_port</code> Port exposed by the container. No (0, 65536) <code>protocol</code> Protocol for the port. No <code>UDP</code>, <code>TCP</code>, <code>SCTP</code>"},{"location":"user-guide/app-system-descriptions/#componentscontainersenv","title":"<code>components[].containers[].env[]</code>","text":"Field Description Required Allowed Values <code>name</code> Env variable name. No - <code>value</code> Env variable value. No - <code>value_from.field_ref.field_path</code> Reference to Kubernetes field. No -"},{"location":"user-guide/app-system-descriptions/#componentsqos_metrics","title":"<code>components[].qos_metrics[]</code>","text":"Field Description Required Allowed Values <code>application_metric_id</code> App metric id. No - <code>target</code> Metric target value. No - <code>relation</code> Desired relation (metric vs target). No <code>lower_or_equal</code>, <code>greater_or_equal</code>, <code>equal</code>, <code>lower_than</code>, <code>greater_than</code>"},{"location":"user-guide/app-system-descriptions/#other-component-fields","title":"Other Component Fields","text":"Field Description Required Allowed Values <code>host_network</code> Whether to use host network namespace. No <code>True</code>, <code>False</code> <code>runtime_class_name</code> Runtime class to use. No <code>nvidia</code>, <code>default</code>, <code>kata-fc</code>, <code>kata-dragon</code>, <code>urunc</code>, <code>crun</code>, <code>lunatic</code>, <code>nvidia-experimental</code>, <code>spin</code>, <code>wasmedge</code>, <code>slight</code> <code>restart_policy</code> Restart policy for the container. No <code>Always</code>, <code>OnFailure</code>, <code>Never</code> <code>os</code> Operating system type. No <code>ubuntu</code>, <code>kali</code>, <code>zephyr</code> <code>node_type</code> Type of the host node. No <code>virtualized</code>, <code>native</code>, <code>bare_metal</code> <code>container_runtime</code> Container runtime. No <code>containerd</code>, <code>docker</code>, <code>emb_serve</code>"},{"location":"user-guide/app-system-descriptions/#component_interactions","title":"<code>component_interactions[]</code>","text":"Field Description Required Allowed Values <code>component_name1</code> Source component. No - <code>component_name2</code> Destination component. No - <code>type</code> Type of interaction. No <code>ingress</code>, <code>egress</code>"},{"location":"user-guide/app-system-descriptions/#global_satisfaction","title":"<code>global_satisfaction</code>","text":"Field Description Required Allowed Values <code>threshold</code> Minimum required satisfaction score. No [0.0, 1] <code>relation</code> Satisfaction comparison. No <code>greater_or_equal</code>, <code>equal</code>, <code>greater_than</code> <code>achievement_weights[].metric_id</code> Metric used for satisfaction. No - <code>achievement_weights[].weight</code> Weight of each metric (total weight sum must be 1). No -"},{"location":"user-guide/app-system-descriptions/#continuum-cluster-and-node-custom-resources-reference","title":"Continuum, Cluster, and Node Custom Resources Reference","text":"<p>This documentation provides full example YAMLs and hierarchical field reference tables for the following custom resource definitions:</p> <ul> <li><code>MLSysOpsContinuum</code></li> <li><code>MLSysOpsCluster</code></li> <li><code>MLSysOpsNode</code></li> </ul> <p>Each section includes: - A sample YAML snippet. - A structured table of fields with descriptions, required/optional status, and allowed values (if defined).</p>"},{"location":"user-guide/app-system-descriptions/#mlsysopscontinuum","title":"MLSysOpsContinuum","text":""},{"location":"user-guide/app-system-descriptions/#example-continuumyaml","title":"Example <code>continuum.yaml</code>","text":"<pre><code>MLSysOpsContinuum:\n  name: demo-continuum\n  continuum_id: demo-cont-id\n  clusters:\n    - uth-prod-cluster\n</code></pre>"},{"location":"user-guide/app-system-descriptions/#field-reference-table","title":"Field Reference Table","text":"Field Description Required Allowed Values <code>name</code> The continuum slice name. Yes - <code>continuum_id</code> The unique continuum identifier. Yes - <code>clusters</code> The set of registered clusters. Yes - ---"},{"location":"user-guide/app-system-descriptions/#mlsysopscluster","title":"MLSysOpsCluster","text":""},{"location":"user-guide/app-system-descriptions/#example-clusteryaml","title":"Example <code>cluster.yaml</code>","text":"<pre><code>MLSysOpsCluster:\n  name: uth-prod-cluster\n  cluster_id: uth-prod-cluster\n  nodes:\n    - node-1\n    - node-2\n    - node-3\n</code></pre>"},{"location":"user-guide/app-system-descriptions/#field-reference-table_1","title":"Field Reference Table","text":"Field Description Required Allowed Values <code>name</code> The cluster name. Yes - <code>cluster_id</code> The unique continuum identifier. Yes - <code>nodes</code> The set of registered nodes. Yes - ---"},{"location":"user-guide/app-system-descriptions/#mlsysopsnode","title":"MLSysOpsNode","text":""},{"location":"user-guide/app-system-descriptions/#example-nodeyaml","title":"Example <code>node.yaml</code>","text":"<pre><code>MLSysOpsNode:\n  name: node-1\n  labels:\n    - gpu\n    - edge-ready\n  continuum_layer: edge\n  cluster_id: uth-prod-cluster\n  mobile: False\n  location: [22.9576, 40.6401]  # [longitude, latitude] for stationary nodes\n  sensors:\n    - camera:\n        model: d455\n        camera_type: rgb\n        framerate: 30\n        supported_resolutions: [\"1024x768\"]\n    - temperature:\n        model: sdc30\n  environment:\n    node_type: virtualized\n    os: ubuntu\n    container_runtime: [\"containerd\"]\n  hardware:\n    cpu:\n      model: Intel-i7\n      architecture: amd64\n      frequency: [2400000000, 3000000000]\n      performance_indicator: 75 # BogoMIPS\n    memory: 16\n    disk: \"256\"\n    gpu:\n      model: k80\n      memory: \"4\"\n      performance_indicator: 100 \n</code></pre>"},{"location":"user-guide/app-system-descriptions/#field-reference-table_2","title":"Field Reference Table","text":"Field Description Required Allowed Values <code>name</code> The name of the node. No - <code>labels</code> The required labels for filtering. No - <code>continuum_layer</code> Continuum placement level. Yes <code>cloud</code>, <code>edge_infrastructure</code>, <code>edge</code>, <code>far_edge</code> <code>cluster_id</code> The unique cluster identifier that the node reports to. No - <code>mobile</code> Specify if the node is mobile or stationary. No - <code>location</code> Geolocation coordinates (lon, lat). Valid only for stationary nodes. For mobile ones, the respective information is collected using telemetry. No - <code>sensors[].camera.model</code> The model name of the camera sensor. No <code>imx415</code>, <code>imx219</code>, <code>d455</code>, <code>imx477</code>, <code>picamera-v2</code> <code>sensors[].camera.camera_type</code> The camera sensor type. No - <code>sensors[].camera.framerate</code> Framerate. No - <code>sensors[].camera.supported_resolutions</code> Supported camera resolutions. No <code>1024x768</code>, <code>4056x3040</code> <code>sensors[].temperature.model</code> The model name of the temperature sensor. No <code>sdc30</code>, <code>ds18b20</code> <code>environment.node_type</code> Node type. Yes <code>virtualized</code>, <code>native</code>, <code>bare_metal</code> <code>environment.os</code> Operating system. Yes <code>ubuntu</code>, <code>kali</code>, <code>zephyr</code> <code>environment.container_runtime[]</code> Supported runtimes. Yes <code>containerd</code>, <code>docker</code>, <code>emb_serve</code> <code>hardware.cpu.model</code> CPU model name. No - <code>hardware.cpu.architecture</code> CPU architecture. No <code>amd64</code>, <code>arm64</code> <code>hardware.cpu.frequency[]</code> Possible CPU frequency values (Hz). No - <code>hardware.cpu.performance_indicator</code> Quantifies processing capabilities (BogoMIPS). No - <code>hardware.memory</code> Memory size (GB). No - <code>hardware.disk</code> Disk space (GB). No - <code>hardware.gpu.model</code> GPU model. No <code>k80</code>, <code>k40</code> <code>hardware.gpu.memory</code> GPU memory size. No - <code>hardware.gpu.performance_indicator</code> GPU performance score. No - ---"},{"location":"user-guide/mechanism-implementation/","title":"Mechanisms","text":""},{"location":"user-guide/mechanism-implementation/#fluidity","title":"Fluidity","text":"<ul> <li>In this version, the supported actions are: (i) <code>deploy</code>, (ii) <code>move</code>,  (iii) <code>remove</code>, (iv) <code>change_spec</code>. Below we document the expected format of each action.</li> <li>Deploy: <code>{'action': 'deploy', 'host': node_name}</code> (deploy component on node with hostname <code>node_name</code>, which can  also be retrieved from the respective MLSysOpsNode description).</li> <li>Move: <code>{'action': 'move', 'target_host': node_name_1, 'src_host': node_name_2}</code>  (relocate component from <code>node_name_2</code> to <code>node_name_1</code>).</li> <li>Remove: <code>{'action': 'remove', 'host': node_name}</code> (remove component from node with hostname <code>node_name</code>).</li> <li> <p>Change spec: <code>{'action': 'change_spec', 'new_spec': updated_spec, 'host': node_name}</code> (Change component specification on node <code>node_name</code> to the new spec <code>updated_spec</code> that follows the same structure as the respective component description that are used in MLSysOpsApp resources).</p> </li> <li> <p>The <code>new_plan</code> dictionary must be comprised of the the following keys:</p> <ul> <li><code>deployment_plan</code>: It is the desired plan to be executed by Fluidity. The value is also a  dictionary with the component names as keys (only those that adaptation should occur)  and the respective value is a list of actions to be made for this component (see above).</li> <li><code>initial_plan</code>: A boolean flag, indicating whether the plan refers to the initial deployment of the component(s) or not.</li> </ul> </li> </ul> <p>Also refer to policy plugins and policy implementation docs.</p>"},{"location":"user-guide/policy-implementation/","title":"Initial deployment and adaptation policy reference","text":"<p>This section provides three indicative examples of different policies for (i) static component placement, (ii) component relocation and (iii) component specification modification. Also refer to Policy plugins doc.</p>"},{"location":"user-guide/policy-implementation/#example-policy-staticplacementpy","title":"Example <code>policy-staticPlacement.py</code>","text":"<p><code>initial_plan()</code> reads the application description and places the components to the respective hosts if specified. If the none of the components has fixed node requirements, an empty plan is returned.</p> <pre><code>\"\"\" Plugin function to implement the initial deployment logic.\n\"\"\"\ndef initial_plan(context, app_desc, system_desc):\n    # Store app name and description in context\n    context['name'] = app_desc['name']\n    context['spec'] = app_desc['spec']\n\n    # Set initial plan flag to True so that analyze() can trigger plan()\n    if 'initial_deployment_finished' not in context:\n        context['initial_deployment_finished'] = True\n\n    # Store app component names\n    context['component_names'] = []\n    plan = {}\n\n    for component in app_desc['spec']['components']:\n\n        comp_name = component['metadata']['name']\n        logger.info('component %s', comp_name)\n        context['component_names'].append(comp_name)\n        node_placement = component.get(\"node_placement\", None)\n        if node_placement:\n            node_name = node_placement.get(\"node\", None)\n            # If node has static placement requirement, add action to plan\n            if node_name:\n                plan[comp_name] = [{'action': 'deploy', 'host': node_name}]\n\n    return plan, context\n</code></pre> <p><code>analyze()</code> just triggers <code>plan()</code> at the first invocation. All the subsequent calls will return False. <pre><code>async def analyze(context, application_description, system_description, mechanisms, telemetry, ml_connector):\n    # Retrieve the first application from the list\n    application = application_description[0]\n    adaptation = False\n\n    # The first time that analyze is called, set flag to True\n    if 'initial_deployment_finished' not in context:\n        logger.info('initial deployment not finished')\n        adaptation = True\n\n    return adaptation, context\n</code></pre> If <code>initial_plan()</code> has returned the first plan, return it to Fluidity for execution. Otherwise, no  adaptation occurs (empty plan). <pre><code>async def plan(context, application_description, system_description, mechanisms, telemetry, ml_connector):\n    plan_result = {}\n    plan_result['deployment_plan'] = {}\n    application = application_description[0]\n    description_changed = False\n\n\n    if 'initial_deployment_finished' not in context:\n        initial_plan_result, new_context = initial_plan(context, application, system_description)\n        if initial_plan_result:\n            plan_result['deployment_plan'] = initial_plan_result\n            plan_result['deployment_plan']['initial_plan'] = True\n\n    if plan_result['deployment_plan']:\n        plan_result['name'] = context['name']\n\n    new_plan = {\n        \"fluidity\": plan_result\n    }\n    logger.info('plan: New plan %s', new_plan)\n\n    return new_plan, context\n</code></pre></p>"},{"location":"user-guide/policy-implementation/#example-policy-relocatecomponentspy","title":"Example <code>policy-relocateComponents.py</code>","text":"<p>This policy relocates (deploys a new component instance on a host and removes the old one), for demo purposes, based on custom logic that invokes <code>plan()</code> with configurable frequency. This happens only for components that do not have strict placement requirements (node is not specified via the app description).</p> <p>Setup the initial context of the policy using <code>initialize()</code> function. <pre><code>def initialize():\n    print(f\"Initializing policy {inspect.stack()[1].filename}\")\n\n    initialContext = {\n        \"telemetry\": {\n            \"metrics\": [\"node_load1\"],\n            \"system_scrape_interval\": \"5s\"\n        },\n        \"mechanisms\": [\n            \"fluidity_proxy\"\n        ],\n        \"packages\": [],\n        \"configuration\": {\n            \"analyze_interval\": \"10s\"\n        },\n        \"latest_timestamp\": None,\n        \"core\": False,\n        \"scope\": \"application\",\n        \"current_placement\": None,\n        \"initial_deployment_finished\": False,\n        \"moving_interval\": \"30s\",\n        \"dynamic_placement_comp\": None\n    }\n\n    return initialContext\n</code></pre></p> <p><code>parse_analyze_interval()</code> converts the key stored in context to seconds in order to manually set the frequency of <code>plan()</code> invocation. <pre><code>def parse_analyze_interval(interval: str) -&gt; int:\n    \"\"\"\n    Parses an analyze interval string in the format 'Xs|Xm|Xh|Xd' and converts it to seconds.\n\n    Args:\n        interval (str): The analyze interval as a string (e.g., \"5m\", \"2h\", \"1d\").\n\n    Returns:\n        int: The interval in seconds.\n\n    Raises:\n        ValueError: If the format of the interval string is invalid.\n    \"\"\"\n    # Match the string using a regex: an integer followed by one of s/m/h/d\n    match = re.fullmatch(r\"(\\d+)([smhd])\", interval)\n    if not match:\n        raise ValueError(f\"Invalid analyze interval format: '{interval}'\")\n\n    # Extract the numeric value and the time unit\n    value, unit = int(match.group(1)), match.group(2)\n\n    # Convert to seconds based on the unit\n    if unit == \"s\":  # Seconds\n        return value\n    elif unit == \"m\":  # Minutes\n        return value * 60\n    elif unit == \"h\":  # Hours\n        return value * 60 * 60\n    elif unit == \"d\":  # Days\n        return value * 24 * 60 * 60\n    else:\n        raise ValueError(f\"Unsupported time unit '{unit}' in interval: '{interval}'\")\n</code></pre></p> <p><code>initial_plan()</code> checks for components without fixed placement requirements and produces the initial deployment plan.</p> <pre><code>\"\"\" Plugin function to implement the initial deployment logic.\n\"\"\"\ndef initial_plan(context, app_desc, system_description):\n    logger.info('initial deployment phase ', app_desc)\n\n    context['name'] = app_desc['name']\n    context['spec'] = app_desc['spec']\n    context['initial_deployment_finished'] = True\n    context['component_names'] = []\n    plan = {}\n\n    # Random host selection to relocate between two nodes of the cluster\n    context['main_node'] = system_description['MLSysOpsCluster']['nodes'][0]\n    context['alternative_node'] = system_description['MLSysOpsCluster']['nodes'][1]\n    # Retrieve the first node of the node list.\n    context[\"current_placement\"] = system_description['MLSysOpsCluster']['nodes'][0]\n\n    for component in app_desc['spec']['components']:\n        comp_name = component['metadata']['name']\n        logger.info('component %s', comp_name)\n        context['component_names'].append(comp_name)\n        node_placement = component.get(\"node_placement\")\n        if node_placement:\n            node_name = node_placement.get(\"node\", None)\n            if node_name:\n                logger.info('Found node name. Will continue')\n                continue\n        context['dynamic_placement_comp'] = comp_name\n        plan[comp_name] = [{'action': 'deploy', 'host': context[\"current_placement\"]}]\n    logger.info('Initial plan %s', plan)\n    return plan, context\n</code></pre> <p><code>analyze()</code> periodically triggers adaptation based on manual configuration in <code>context['moving_interval']</code>. <pre><code>async def analyze(context, application_description, system_description, mechanisms, telemetry, ml_connector):\n    logger.info(f\"\\nTelemetry {telemetry}\")\n\n    current_timestamp = time.time()\n\n    # The first time called\n    if context['latest_timestamp'] is None:\n        context['latest_timestamp'] = current_timestamp\n        return True, context\n\n    # All the next ones, get it\n    analyze_interval = parse_analyze_interval(context['moving_interval'])\n    if current_timestamp - context['latest_timestamp'] &gt; analyze_interval:\n        context['latest_timestamp'] = current_timestamp\n        return True, context\n\n    return False, context\n</code></pre></p> <p><code>plan()</code> checks the current host and relocates to the other node. <pre><code>async def plan(context, application_description, system_description, mechanisms, telemetry, ml_connector):\n    #logger.info(f\"Called relocation plan  ----- {mechanisms}\")\n\n    context['initial_plan'] = False\n\n    plan_result = {}\n    plan_result['deployment_plan'] = {}\n    application = application_description[0]\n\n    if 'initial_deployment_finished' in context and context['initial_deployment_finished'] == False:\n        initial_plan_result, new_context = initial_plan(context, application, system_description)\n        if initial_plan_result:\n            plan_result['deployment_plan'] = initial_plan_result\n            plan_result['deployment_plan']['initial_plan'] = True\n\n            comp_name = new_context['dynamic_placement_comp']\n    else:\n        comp_name = context['dynamic_placement_comp']\n        plan_result['deployment_plan']['initial_plan'] = False\n        plan_result['deployment_plan'][comp_name] = []\n        curr_plan = {}\n\n        if context['main_node'] == context[\"current_placement\"]:\n            curr_plan = {\n                \"action\": \"move\",\n                \"target_host\": context['alternative_node'],\n                \"src_host\": context['main_node'],\n            }\n            context[\"current_placement\"] = context['alternative_node']\n        elif context['alternative_node'] == context[\"current_placement\"]:\n            curr_plan = {\n                \"action\": \"move\",\n                \"target_host\": context['main_node'],\n                \"src_host\": context['alternative_node'],\n            }\n            context[\"current_placement\"] = context['main_node']\n\n        plan_result['deployment_plan'][comp_name].append(curr_plan)\n\n\n    if plan_result:\n        plan_result['name'] = context['name']\n\n    new_plan = {\n        \"fluidity\": plan_result,\n    }\n    logger.info('plan: New plan %s', new_plan)\n\n    return new_plan, context\n</code></pre></p>"},{"location":"user-guide/policy-implementation/#example-policy-changecompspecpy","title":"Example <code>policy-changeCompSpec.py</code>","text":"<p>This policy performs component specification change at runtime. We showcase 3 different changes based on: (i) Kubernetes Pod runtime class name. (ii) Container image used. (iii) Pod resource requirements (cpu and memory).</p> <pre><code>spec_changes = cycle([\n    {'runtime_class_name': cycle(['crun', 'nvidia'])},\n    {'image': cycle(['harbor.nbfc.io/mlsysops/test-app:sha-90e0077', 'harbor.nbfc.io/mlsysops/test-app:latest'])},\n    {'platform_requirements': {\n            'cpu': { \n                'requests': '', # in m\n                'limits': '' # in m\n            },\n            'memory': {\n                'requests':  '', # in Mi\n                'limits':  '' # in Mi\n            }\n        }\n    }\n])\n</code></pre> <pre><code>def initialize():\n    print(f\"Initializing policy {inspect.stack()[1].filename}\")\n\n    initialContext = {\n        \"telemetry\": {\n            \"metrics\": [\"node_load1\"],\n            \"system_scrape_interval\": \"1s\"\n        },\n        \"mechanisms\": [\n            \"fluidity_proxy\"\n        ],\n        \"packages\": [],\n        \"configuration\": {\n            \"analyze_interval\": \"30s\"\n        },\n        \"latest_timestamp\": None,\n        \"core\": False,\n        \"scope\": \"application\",\n        \"curr_comp_idx\": 0,\n        \"current_placement\": None,\n        \"initial_deployment_finished\": False,\n        \"moving_interval\": \"30s\",\n        \"dynamic_placement_comp\": None\n    }\n\n    return initialContext\n</code></pre> <pre><code>async def analyze(context, application_description, system_description, mechanisms, telemetry, ml_connector):\n    current_timestamp = time.time()\n\n    # The first time called\n    if context['latest_timestamp'] is None:\n        context['latest_timestamp'] = current_timestamp\n        return True, context\n\n    # All the next ones, get it\n    analyze_interval = parse_analyze_interval(context['moving_interval'])\n    logger.info(f\"{current_timestamp} - {context['latest_timestamp']}  = {current_timestamp - context['latest_timestamp']} with interval {analyze_interval}\")\n\n    if current_timestamp - context['latest_timestamp'] &gt; analyze_interval:\n        context['latest_timestamp'] = current_timestamp\n        return True, context\n\n    return True, context\n</code></pre> <p><code>plan()</code> selects one of the available changes in the component spec (round-robin). We show change between 2 runtime class names, 2 container images and random component  resource requirements. <pre><code>async def plan(context, application_description, system_description, mechanisms, telemetry, ml_connector):\n    plan_result = {}\n    plan_result['deployment_plan'] = {}\n    application = application_description[0]\n    description_changed = False\n    change_idx = cycle([0, 1, 2])\n    curr_change = next(spec_changes)\n    cpu_suffix = 'm'\n    mem_suffix = 'Mi'\n\n    # Get the first component just for demo purposes\n    component = application['spec']['components'][0]\n    comp_name = component['metadata']['name']\n    logger.info(f'component spec {component}')\n\n    # If the component has fixed node placement requirement find the host\n    # else select the first node\n    if 'node_placement' in component and 'node' in component['node_placement']:\n        node = component['node_placement']['node']\n        logger.info(f'Found static placement on {node} for comp {comp_name}')\n    else: \n        node = system_description['MLSysOpsCluster']['nodes'][0]\n        logger.info(f'Randomly select host {node} for {comp_name}')\n\n    plan_result['deployment_plan'][comp_name] = []\n\n    for key in curr_change:\n        if key == 'runtime_class_name': \n            component[key] = next(curr_change[key])\n        else:\n            for container in component['containers']:\n\n                if key == 'image':\n                    # Find the next image to be used and continue\n                    container[key] = next(curr_change[key])\n                    continue\n\n                # Set random cpu/mem requirements for the component\n                request_cpu = str(random.randint(0, 300))\n                limit_cpu = str(random.randint(301, 400))\n\n                request_mem = str(random.randint(0, 300))\n                limit_mem = str(random.randint(301, 400))\n\n                logger.info(f'request_cpu+cpu_suffix {request_cpu+cpu_suffix}')\n\n                if key not in container or 'cpu' not in container[key] or 'memory' not in container[key]:\n                    container[key] = {\n                        'cpu': {\n                            'requests': '',\n                            'limits': ''\n                        },\n                        'memory': {\n                            'requests': '',\n                            'limits': ''\n                        }\n                    }\n\n                container[key]['cpu']['requests'] = request_cpu+cpu_suffix\n                container[key]['cpu']['limits'] = limit_cpu+cpu_suffix\n\n                container[key]['memory']['requests'] = request_mem+mem_suffix\n                container[key]['memory']['limits'] = limit_mem+mem_suffix\n\n        plan_result['deployment_plan'][comp_name].append({'action': 'change_spec', 'new_spec': component, 'host': node})\n        logger.info(f\"Applying change type {key} to comp {comp_name}, new spec is {component}\")\n\n    # If there is a produced plan, extend the plan accordingly with the application name and initial_plan flag\n    if plan_result:\n        plan_result['name'] = application['name']\n        # This policy will only take effect after initial deployment is done.\n        plan_result['deployment_plan']['initial_plan'] = False\n\n    new_plan = {\n        \"fluidity\": plan_result\n    }\n    logger.info('plan: New plan %s', new_plan)\n\n    return new_plan, context\n</code></pre></p>"}]}